{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450adfb1",
   "metadata": {},
   "source": [
    "# **Mengimport Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60bddf",
   "metadata": {},
   "source": [
    "Mengimport libraries yang diperlukan seperti torch, torchvision pil, dll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59bce78c-accd-4577-8207-3071c2446617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights, mobilenet_v3_large, MobileNet_V3_Large_Weights, mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import tqdm\n",
    "import random \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import cv2\n",
    "import imageio\n",
    "from facenet_pytorch import MTCNN\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import MTCNN\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb38fb",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831d086",
   "metadata": {},
   "source": [
    "Membuat kelas preprocessing menggunakan MTCNN yang dimana juga ditambahkan rotasi gambar biar wajahnya sejajar dan membuat subfolder untuk menyimpan gambar setiap kelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97f8da2-77ee-4175-9f28-50a5a580ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "class FaceCropper:\n",
    "    def __init__(self, mtcnn_device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.mtcnn = MTCNN(keep_all=False, device=mtcnn_device)\n",
    "        self.yolo_fallback = YOLO(\n",
    "            r\"D:\\Models\\huggingface\\hub\\models--arnabdhar--YOLOv8-Face-Detection\\snapshots\\52fa54977207fa4f021de949b515fb19dcab4488\\model.pt\"\n",
    "        )\n",
    "\n",
    "    def extract_class(self, txt):\n",
    "        raw_class = re.split(r\"[-_.]\", txt, 2)[0:2]\n",
    "        complete_class = raw_class[0] + \"_\" + raw_class[1]\n",
    "        complete_class = re.sub(r'[\\d\\s]+$', '', complete_class)\n",
    "        return complete_class\n",
    "    \n",
    "    def auto_rotate_image(self, image):\n",
    "        \"\"\"Rotate image using MTCNN landmarks\"\"\"\n",
    "        _, _, landmarks = self.mtcnn.detect(image, landmarks=True)\n",
    "        if landmarks is not None:\n",
    "            left_eye, right_eye = landmarks[0][:2]\n",
    "            dx, dy = right_eye[0] - left_eye[0], right_eye[1] - left_eye[1]\n",
    "            angle = math.degrees(math.atan2(dy, dx))\n",
    "            image = image.rotate(-angle, expand=True)\n",
    "        return image\n",
    "    \n",
    "    def crop_face(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image = ImageOps.exif_transpose(image)\n",
    "            image = self.auto_rotate_image(image)\n",
    "    \n",
    "            # First try MTCNN\n",
    "            boxes, _ = self.mtcnn.detect(image)\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                box = boxes[0]\n",
    "            else:\n",
    "                # Fall back to YOLO\n",
    "                results = self.yolo_fallback(image_path)\n",
    "                if results and len(results[0].boxes.xyxy) > 0:\n",
    "                    box = results[0].boxes.xyxy[0].cpu().numpy()\n",
    "                else:\n",
    "                    return None\n",
    "    \n",
    "            left, top, right, bottom = map(int, box)\n",
    "            return image.crop((left, top, right, bottom))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_folder(self, input_dir, output_dir, batch_size=32, max_workers=8):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_files = image_files[i:i + batch_size]\n",
    "        \n",
    "            def process_file(fname):\n",
    "                path = os.path.join(input_dir, fname)\n",
    "                cropped = self.crop_face(path)\n",
    "                if cropped:\n",
    "                    class_name = self.extract_class(fname)\n",
    "                    class_dir = os.path.join(output_dir, class_name)\n",
    "                    os.makedirs(class_dir, exist_ok=True)  # Create class subfolder safely\n",
    "                    save_path = os.path.join(class_dir, fname)  # keep original filename\n",
    "                    # If file exists, append counter\n",
    "                    counter = 1\n",
    "                    base, ext = os.path.splitext(save_path)\n",
    "                    while os.path.exists(save_path):\n",
    "                        save_path = f\"{base}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "                    cropped.save(save_path)\n",
    "        \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                list(tqdm(executor.map(process_file, batch_files), total=len(batch_files)))\n",
    "        \n",
    "            # Clean memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf0281",
   "metadata": {},
   "source": [
    "Menggunakan kelas preprocessing yang sudah dibuat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68feb7a-e0b8-4662-9421-5304bc30e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Starting TRAIN image cropping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:10<00:00,  3.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:22<00:00,  2.58s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:15<00:00,  2.10it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:19<00:00,  1.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.75it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:29<00:00,  2.80s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:43<00:00,  1.35s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.28it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  5.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.64it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:16<00:00,  1.89it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:55<00:00,  1.73s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [03:04<00:00,  5.75s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:41<00:00,  1.28s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:10<00:00,  3.09it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:09<00:00,  3.47it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:26<00:00,  1.22it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:35<00:00,  4.87s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [06:36<00:00, 12.38s/it]  \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.56it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:38<00:00,  1.22s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.78it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.76it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:30<00:00,  1.04it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:15<00:00,  2.01it/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 15/32 [00:11<00:16,  1.06it/s]"
     ]
    }
   ],
   "source": [
    "# Inisiasi cropper\n",
    "cropper = FaceCropper()\n",
    "\n",
    "# --- Train paths ---\n",
    "train_input = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train\"\n",
    "train_output = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "\n",
    "print(\"ðŸŸ¢ Starting TRAIN image cropping...\")\n",
    "cropper.process_folder(train_input, train_output)\n",
    "\n",
    "# --- Test paths ---\n",
    "test_input = train_input.replace(\"Train\", \"Test\")\n",
    "test_output = train_output.replace(\"Train\", \"Test\")\n",
    "\n",
    "print(\"\\nðŸŸ£ Starting TEST image cropping...\")\n",
    "cropper.process_folder(test_input, test_output)\n",
    "\n",
    "print(\"\\nâœ… All done! Cropped images saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262a0b9",
   "metadata": {},
   "source": [
    "# **Membuat kelas ArcFaceLoss sebagai loss dari model nanti**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da78794",
   "metadata": {},
   "source": [
    "ArcFaceLoss dipilih karena untuk face recognition arcfaceloss yang memberikan generelesasi paling baik dan tidak overfitting selain itu convergenya cepet tidak seperti Triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956a95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33daeb65",
   "metadata": {},
   "source": [
    "# **Membuat MobilenetEmbedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7ff164",
   "metadata": {},
   "source": [
    "Membuat embedding face recognition menggunakan mobilenet, dimana hasil terbaik ternyata waktu weightnya tidak di freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa91cec-5d77-4431-9d41-dbb769f486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobilenetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128, weights=None):\n",
    "        super().__init__()\n",
    "        # self.mobilenet = mobilenet_v3_large(weights=weights)\n",
    "        self.mobilenet = mobilenet_v2(weights=weights) \n",
    "        self.mobilenet.classifier[1] = nn.Linear(1280, embedding_size)\n",
    "        # if weights is not None:\n",
    "        #     for param in self.mobilenet.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        #     for param in self.mobilenet.classifier[1].parameters():\n",
    "        #         param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mobilenet(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774529b0",
   "metadata": {},
   "source": [
    "## **Training model mobilenetv2_arcface_best**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba6484",
   "metadata": {},
   "source": [
    "Kita memasukan dataset ke class datasets menggunakan transformasi yang dimana trainingnya itu ditambahkan beberapa data augmentasi karena sebelum ini masih overfit dan untuk realtime tidak terlalu bagus lalu dimasukan ke-dataloader dan training dimulai. Disimpan model weightnya saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43be511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.73it/s]\n",
      "Epoch 1/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 11.1741 | Test Loss: 9.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.77it/s]\n",
      "Epoch 2/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.1952 | Test Loss: 4.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.76it/s]\n",
      "Epoch 3/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 1.1857 | Test Loss: 2.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 4/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.5585 | Test Loss: 1.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.76it/s]\n",
      "Epoch 5/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.2898 | Test Loss: 1.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 6/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.1331 | Test Loss: 0.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 7/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.0738 | Test Loss: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:22<00:00,  1.80it/s]\n",
      "Epoch 8/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.0624 | Test Loss: 0.8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.74it/s]\n",
      "Epoch 9/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.0569 | Test Loss: 0.7826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 10/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.0516 | Test Loss: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 11/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.0588 | Test Loss: 0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 12/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.0553 | Test Loss: 0.6495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 13/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.0307 | Test Loss: 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.73it/s]\n",
      "Epoch 14/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.0381 | Test Loss: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 15/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.0538 | Test Loss: 0.7997\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.3,\n",
    "    scale=30.0\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 15\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48965d6c",
   "metadata": {},
   "source": [
    "## **Training mobilenetv2_arcface_diffmargin_moreagument_best.pth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37789dc9",
   "metadata": {},
   "source": [
    "Sama seperti sebelumnya tetapi margin dari arcfacelossnya ditambahkan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 17.7598 | Test Loss: 16.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 13.3814 | Test Loss: 13.4777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 7.8086 | Test Loss: 9.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.71it/s]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.9763 | Test Loss: 6.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 2.0972 | Test Loss: 4.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.68it/s]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.1835 | Test Loss: 3.5472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.55it/s]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.8025 | Test Loss: 2.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.58it/s]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.5074 | Test Loss: 2.6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.58it/s]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.4743 | Test Loss: 1.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.2710 | Test Loss: 1.9759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.71it/s]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2281 | Test Loss: 1.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.1962 | Test Loss: 2.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.64it/s]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.1630 | Test Loss: 1.5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.1556 | Test Loss: 1.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.1303 | Test Loss: 1.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 16/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.0919 | Test Loss: 1.3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 17/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.1293 | Test Loss: 1.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 18/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.1414 | Test Loss: 1.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.65it/s]\n",
      "Epoch 19/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.1005 | Test Loss: 1.4541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.54it/s]\n",
      "Epoch 20/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 0.0809 | Test Loss: 1.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.59it/s]\n",
      "Epoch 21/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 0.0490 | Test Loss: 1.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 22/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 0.0544 | Test Loss: 1.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:29<00:00,  1.39it/s]\n",
      "Epoch 23/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 0.0875 | Test Loss: 1.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 24/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 0.1289 | Test Loss: 1.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.65it/s]\n",
      "Epoch 25/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 0.0814 | Test Loss: 1.2364\n",
      "Early stopping triggered after 25 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb214f3",
   "metadata": {},
   "source": [
    "## **Training mobilenetv2_arcface_diffmargin_moreagument_best_0-2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02525c",
   "metadata": {},
   "source": [
    "Marginya sama sebelumnya, tetapi transformationya ditambah karena masih overfit lossnya. Melatih 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1672f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 18.0244 | Test Loss: 16.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:28<00:00,  1.43it/s]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 12.8248 | Test Loss: 12.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:30<00:00,  1.35it/s]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.9796 | Test Loss: 7.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.6455 | Test Loss: 4.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 1.3765 | Test Loss: 2.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.64it/s]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.0202 | Test Loss: 2.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.63it/s]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.6132 | Test Loss: 1.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.5023 | Test Loss: 2.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.4219 | Test Loss: 1.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.57it/s]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.3444 | Test Loss: 1.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2669 | Test Loss: 1.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.53it/s]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.4428 | Test Loss: 1.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.57it/s]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.4278 | Test Loss: 1.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.55it/s]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.3367 | Test Loss: 1.3939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.3294 | Test Loss: 1.3997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:28<00:00,  1.42it/s]\n",
      "Epoch 16/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.3273 | Test Loss: 1.5632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.53it/s]\n",
      "Epoch 17/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.2587 | Test Loss: 1.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.49it/s]\n",
      "Epoch 18/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.1835 | Test Loss: 1.6515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.50it/s]\n",
      "Epoch 19/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.2681 | Test Loss: 1.8248\n",
      "Early stopping triggered after 19 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## INI NYOBA MENAMBAH transformation\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V2\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10**LR_MIN_EXP, 10**LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10**WD_MIN_EXP, 10**WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553d562",
   "metadata": {},
   "source": [
    "# **Evaluasi model embedding menggunakan cosine similarity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b2a63",
   "metadata": {},
   "source": [
    "Karena model bukan classifier langsung 54 kelas, jadi kita masih harus kalau ga pake model ml lagi menggunakan ml lain, atau kita tambahkan fully connected diakhir lalu embeddingnya kita freeze weightnya, atau kita menggunakan cosine similarity. Saya memilih cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13bbb233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… mobilenetv2_arcface_best.pth | Train Acc: 100.00% | Test Acc: 96.64%\n",
      "âœ… mobilenetv2_arcface_diffmargin_moreagument_best.pth | Train Acc: 100.00% | Test Acc: 97.57%\n",
      "âœ… mobilenetv2_arcface_diffmargin_moreagument_best_1.pth | Train Acc: 100.00% | Test Acc: 97.57%\n",
      "âœ… mobilenetv2_arcface_diffmargin_moreagument_best_2.pth | Train Acc: 100.00% | Test Acc: 95.25%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(embedding_size=128).to(device)\n",
    "model_list = [r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\",\n",
    "              r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\", \n",
    "              r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_1.pth\" , \n",
    "              r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\"]\n",
    "for i in model_list:\n",
    "    # model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\", weights_only=True))\n",
    "    model.load_state_dict(torch.load(i, weights_only=True))\n",
    "    # model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\", weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # --- Datasets ---\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "    # --- Dataloaders ---\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # --- Extract train embeddings ---\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb = model(imgs)\n",
    "            X_train.append(emb.cpu().numpy())\n",
    "            y_train.append(labels.numpy())\n",
    "\n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.hstack(y_train)\n",
    "\n",
    "    # --- Extract test embeddings ---\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            emb = model(imgs)\n",
    "            X_test.append(emb.cpu().numpy())\n",
    "            y_test.append(labels.numpy())\n",
    "\n",
    "    X_test = np.vstack(X_test)\n",
    "    y_test = np.hstack(y_test)\n",
    "\n",
    "    # --- Build class prototypes ---\n",
    "    classes = np.unique(y_train)\n",
    "    # Build prototypes with class indices aligned to ImageFolder\n",
    "    prototypes = {}\n",
    "    for idx, cls in enumerate(train_dataset.classes):  # use ImageFolder.classes\n",
    "        class_idx = train_dataset.class_to_idx[cls]\n",
    "        prototypes[class_idx] = X_train[y_train == class_idx].mean(axis=0)\n",
    "        prototypes[class_idx] /= np.linalg.norm(prototypes[class_idx])\n",
    "\n",
    "    # --- Prediction function ---\n",
    "    def predict_embedding(embedding, prototypes, threshold=0.5):\n",
    "        embedding = embedding / np.linalg.norm(embedding)\n",
    "        best_cls = None\n",
    "        best_sim = -1\n",
    "        for cls_idx, proto in prototypes.items():\n",
    "            sim = np.dot(embedding, proto)\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_cls = cls_idx\n",
    "        if best_sim < threshold:\n",
    "            return -1, best_sim  # Unknown\n",
    "        return best_cls, best_sim\n",
    "\n",
    "    # --- Predict on test set ---\n",
    "    predictions = []\n",
    "    similarities = []\n",
    "\n",
    "    for emb in X_test:\n",
    "        cls, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "        predictions.append(cls)\n",
    "        similarities.append(sim)\n",
    "\n",
    "    predictions_test = np.array(predictions)\n",
    "    similarities_test = np.array(similarities)\n",
    "    \n",
    "    # -- Prediction on training set --\n",
    "    predictions = []\n",
    "    similarities = []\n",
    "\n",
    "    for emb in X_train:\n",
    "        cls, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "        predictions.append(cls)\n",
    "        similarities.append(sim)\n",
    "\n",
    "    predictions_train = np.array(predictions)\n",
    "    similarities_train = np.array(similarities)\n",
    "\n",
    "    # --- Accuracy ---\n",
    "    acc_train = np.mean(predictions_train == y_train)\n",
    "    acc_test = np.mean(predictions_test == y_test)\n",
    "    filename = os.path.basename(i)\n",
    "    print(f\"âœ… {filename} | Train Acc: {acc_train*100:.2f}% | Test Acc: {acc_test*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db875f3a",
   "metadata": {},
   "source": [
    "# **Membuat model MobileFaceNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce6fee",
   "metadata": {},
   "source": [
    "Model MobileFaceNet ini terinspirasi basenya jadi mobilenet sama2 ringan, hanya saja ini khusus untuk face recognition embedding menggunakan facearc loss, dan dia trainingnya menggunakan dataset wajah tidak seperti mobilenet yang imagenet dan tidak ada orang lain yang membagi bobot mobilenet ditrain untuk face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "591dee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, 512, bias=False)\n",
    "        self.bn = BatchNorm1d(512)\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Input:\", x.shape)\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        # print(\"After conv1:\", out.shape)\n",
    "        \n",
    "        out = self.conv2_dw(out)\n",
    "        # print(\"After conv2_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "        # print(\"After conv_23:\", out.shape)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        # print(\"After conv_3:\", out.shape)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "        # print(\"After conv_34:\", out.shape)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        # print(\"After conv_4:\", out.shape)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "        # print(\"After conv_45:\", out.shape)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "        # print(\"After conv_5:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "        # print(\"After conv_6_sep:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "        # print(\"After conv_6_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "        # print(\"After conv_6_flatten:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        # print(\"After linear:\", out.shape)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        # print(\"After batchnorm:\", out.shape)\n",
    "\n",
    "        out = l2_norm(out)\n",
    "        # print(\"After l2_norm:\", out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c132224",
   "metadata": {},
   "source": [
    "## **Training MobileFaceNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a857ff31",
   "metadata": {},
   "source": [
    "Training modelnya setelah menemukan lr dan wd yang lumayan bagus dengan epoch sebanyaknya karena menggunakan early stopping. Selain itu saya menyimpan snapshot weight dari model dengan test loss terbaik. Untuk transformasinya kita tambahkan banyak data augmentasi agar tidak overfit, logika training nya sama seperti sebelumnya optimizer pakai adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_13204\\3093528768.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [03:12<00:00,  4.70s/it]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:38<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 16.0721 | Test Loss: 13.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.84s/it]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 9.0412 | Test Loss: 6.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.85s/it]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.5873 | Test Loss: 3.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:19<00:00,  1.93s/it]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:27<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 1.4521 | Test Loss: 1.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:46<00:00,  2.59s/it]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.8508 | Test Loss: 1.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:17<00:00,  1.89s/it]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.6205 | Test Loss: 1.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:14<00:00,  1.82s/it]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:29<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.3514 | Test Loss: 1.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:25<00:00,  2.08s/it]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.2911 | Test Loss: 1.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:17<00:00,  1.89s/it]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.2418 | Test Loss: 1.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.85s/it]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.2387 | Test Loss: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:16<00:00,  1.87s/it]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2610 | Test Loss: 1.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:16<00:00,  1.86s/it]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.2405 | Test Loss: 1.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:09<00:00,  3.15s/it]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:28<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.2368 | Test Loss: 1.4209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:18<00:00,  1.92s/it]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:33<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.1960 | Test Loss: 1.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:45<00:00,  2.57s/it]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.2686 | Test Loss: 1.4817\n",
      "Early stopping triggered after 15 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\n"
     ]
    }
   ],
   "source": [
    "## INI NYOBA MENAMBAH PAKE LAIN\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=512,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10**LR_MIN_EXP, 10**LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10**WD_MIN_EXP, 10**WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c41f73",
   "metadata": {},
   "source": [
    "## **Hasil evaluasi MobileFaceNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eff892b",
   "metadata": {},
   "source": [
    "Akurasinya bukan terbaik tetapi dari terbaik berbeda sedikit, dan ini lebih generelesasi kalau direaltime bener2 bagus dibanding mobilenetembedding. Strategi evaluasinya sama kaya mobilenetembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d70b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_19404\\3016627567.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… mobilenetv2_arcface_diffmargin_moreagument_best.pth | Train Acc: 100.00% | Test Acc: 97.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Model ---\n",
    "# model = MobilenetEmbedding(embedding_size=128).to(device)\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\", weights_only=True))\n",
    "\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "model.eval()\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Extract train embeddings ---\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_train.append(emb.cpu().numpy())\n",
    "        y_train.append(labels.numpy())\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "# --- Extract test embeddings ---\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_test.append(emb.cpu().numpy())\n",
    "        y_test.append(labels.numpy())\n",
    "\n",
    "X_test = np.vstack(X_test)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "# --- Build class prototypes ---\n",
    "classes = np.unique(y_train)\n",
    "# Build prototypes with class indices aligned to ImageFolder\n",
    "prototypes = {}\n",
    "for idx, cls in enumerate(train_dataset.classes):  # use ImageFolder.classes\n",
    "    class_idx = train_dataset.class_to_idx[cls]\n",
    "    prototypes[class_idx] = X_train[y_train == class_idx].mean(axis=0)\n",
    "    prototypes[class_idx] /= np.linalg.norm(prototypes[class_idx])\n",
    "\n",
    "# --- Prediction function ---\n",
    "def predict_embedding(embedding, prototypes, threshold=0.5):\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    best_cls = None\n",
    "    best_sim = -1\n",
    "    for cls_idx, proto in prototypes.items():\n",
    "        sim = np.dot(embedding, proto)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_cls = cls_idx\n",
    "    if best_sim < threshold:\n",
    "        return -1, best_sim  # Unknown\n",
    "    return best_cls, best_sim\n",
    "\n",
    "# --- Predict on test set ---\n",
    "predictions = []\n",
    "similarities = []\n",
    "\n",
    "for emb in X_test:\n",
    "    cls, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "    predictions.append(cls)\n",
    "    similarities.append(sim)\n",
    "\n",
    "predictions_test = np.array(predictions)\n",
    "similarities_test = np.array(similarities)\n",
    "\n",
    "# -- Prediction on training set --\n",
    "predictions = []\n",
    "similarities = []\n",
    "\n",
    "for emb in X_train:\n",
    "    cls, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "    predictions.append(cls)\n",
    "    similarities.append(sim)\n",
    "\n",
    "predictions_train = np.array(predictions)\n",
    "similarities_train = np.array(similarities)\n",
    "\n",
    "# --- Accuracy ---\n",
    "acc_train = np.mean(predictions_train == y_train)\n",
    "acc_test = np.mean(predictions_test == y_test)\n",
    "filename = os.path.basename(i)\n",
    "print(f\"âœ… {filename} | Train Acc: {acc_train*100:.2f}% | Test Acc: {acc_test*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91587d4a",
   "metadata": {},
   "source": [
    "## **Contoh banyak wajah**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e3026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 9 faces.\n"
     ]
    }
   ],
   "source": [
    "class_names = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "normalize_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "mtcnn = MTCNN(\n",
    "    image_size=112, \n",
    "    margin=0, \n",
    "    keep_all=True, \n",
    "    post_process=False, # We will do our own normalization\n",
    "    device=device\n",
    ") \n",
    "\n",
    "# path = r\"D:\\download_d\\IMG_1187.JPG\"\n",
    "# path = r\"D:\\download_d\\IMG_1186.JPG\"\n",
    "# path = r\"D:\\download_d\\IMG_1185.JPG\"\n",
    "# path = r\"D:\\download_d\\IMG_1182.JPG\"\n",
    "# path = r\"D:\\download_d\\IMG_1180.JPG\"\n",
    "# path = r\"D:\\download_d\\IMG_1172.JPG\"\n",
    "path = r\"D:\\download_d\\IMG_1158 (1).JPG\"\n",
    "# Load original image (for display only)\n",
    "try:\n",
    "    cv2_image = cv2.imread(path)\n",
    "    if cv2_image is None:\n",
    "        raise FileNotFoundError(f\"Could not read image from path: {path}\")\n",
    "    original_for_display = cv2_image.copy()  # keep original copy\n",
    "    \n",
    "    # Load PIL image (copy for MTCNN) \n",
    "    pil_image_for_mtcnn = Image.open(path).convert('RGB')\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    sys.exit(0)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred loading the image: {e}\")\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "# --- Detect faces and get aligned tensors ---\n",
    "# This call *already* uses landmarks internally for alignment/rotation\n",
    "face_tensors = mtcnn(pil_image_for_mtcnn) \n",
    "\n",
    "if face_tensors is None:\n",
    "    print(\"No faces detected.\")\n",
    "    sys.exit(0)\n",
    "\n",
    "print(f\"Detected {len(face_tensors)} faces.\")\n",
    "\n",
    "# --- Preprocess face tensors only ---\n",
    "face_tensors = face_tensors.to(device) / 255.0\n",
    "face_tensors = normalize_transform(face_tensors)\n",
    "\n",
    "# --- Get embeddings ---\n",
    "model.eval() # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    embeddings = model(face_tensors)\n",
    "\n",
    "# --- Get boxes AND landmarks for drawing ---\n",
    "# We run detect again, this time asking for landmarks to draw\n",
    "boxes, probs, landmarks = mtcnn.detect(np.array(pil_image_for_mtcnn), landmarks=True) \n",
    "\n",
    "# --- Draw on original image ---\n",
    "if boxes is not None:\n",
    "    # Iterate over boxes and landmarks together\n",
    "    for i, (box, lms) in enumerate(zip(boxes, landmarks)):\n",
    "        emb = embeddings[i].cpu().numpy()\n",
    "        cls_idx, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "        \n",
    "        name = \"Unknown\" if cls_idx == -1 else class_names[cls_idx]\n",
    "        label = f\"{name} ({sim*100:.1f}%)\"\n",
    "        \n",
    "        x1, y1, x2, y2 = [int(b) for b in box]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(original_for_display, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw label background\n",
    "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        cv2.rectangle(original_for_display, (x1, y1 - h - 10), (x1 + w, y1 - 5), (0, 255, 0), -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(original_for_display, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "        \n",
    "        # *** NEW: Draw landmarks ***\n",
    "        for point in lms:\n",
    "            cv2.circle(original_for_display, (int(point[0]), int(point[1])), 2, (0, 0, 255), 2)\n",
    "\n",
    "# --- Show the original image (no preprocessing applied) ---\n",
    "window_name = 'Face Recognition'\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow(window_name, 800, 600) # You can adjust this as needed\n",
    "cv2.imshow(window_name, original_for_display)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615e3f34",
   "metadata": {},
   "source": [
    "# **Realtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78ed3b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "Loading dataset pelatihan untuk membangun prototipe...\n",
      "Ditemukan 54 kelas: ['5221911012_Debora', '5221911025_Anggun', '5231811002_MichaelAndrewDeHaan', '5231811004_Hamdanu Fahmi Utomo', '5231811005_Akhmad Nabil Saputra', '5231811006_Daniel Granesa Kiara', '5231811007_Amalia Dwi Ramadhani', '5231811008_Sophia', '5231811009_Otniel Chresto Purwandi', '5231811010_MEYLAN ARYANI', '5231811013_KusumaRatih', '5231811014_Dian Eka Pratiwi', '5231811015_Fadilah Ratu Azzahra', '5231811016_Kesya', '5231811017_Maulana Ahmad Muhaimin', '5231811018_Sulis Septiani Putri', '5231811019_Chronika', '5231811021_NASHA SHINTA ABP', '5231811022_Lathif Ramadhan', '5231811023_Rahma Fieka Januarni', '5231811024_Maria Febronia Boa', '5231811025_Novera', '5231811026_ULFAH NAFIAH', '5231811027_Naufal', \"5231811028_Ma'ruf Ndaru Sasono\", '5231811029_Andini', '5231811030_Antonia Idan Huler', '5231811031_CindyKusumaningrum', '5231811033_Rama Panji Nararendra Cahaya', '5231811034_Shilsylia Putri Devitasary', '5231811035_Yogi Hanusanjaya', '5231811036_Giffari Riyanda Pradithya', '5231811037_Muhammad Ibra Ramadhon', '5231811038_yudit manda', '5231811039_Rambang Widyadana', '5231911001_Farma', '5231911002_Desak Gde Devika Anantha Armi', '5231911003_ulen', '5231911004_al faisal selan', '5231911005_Deraya', '5231911006_ARI DHARMA DEVANANTA', '5231911007_julia eliza w', '5231911008_Faren', '5231911009_Ica Nurcahya', '5231911010_Jeny Amelindrika Putri', '5231911011_Nirmala Fitri Margitaya', '5231911012_Thalita Revalyna Maharani', '5231911013_REVA SEBRINA SALSABILA', '5231911014_Asti Eka Rahayu', '5231911016_Ameliawati', '5231911017_RAFI AUFA MIRZA', '5231911018_FIRMAN', '5231911019_Glory Valentio Duska Putra', '5231911020_siti nabila maulidya']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_19404\\3347148874.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ekstraksi 2244 embedding pelatihan selesai.\n",
      "Prototipe untuk 54 kelas berhasil dibuat.\n",
      "Memulai stream video... Tekan 'q' untuk keluar.\n",
      "Stream dihentikan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# --- Variabel & Path (dari skrip Anda) ---\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_1.pth\"\n",
    "# model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_1.pth\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# --- Model ---\n",
    "# model = MobilenetEmbedding(embedding_size=128).to(device)\n",
    "# # model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\", weights_only=True))\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Transformasi ---\n",
    "# Transformasi untuk model embedding\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 1: MEMBANGUN PROTOTIPE KELAS (Dijalankan sekali saat startup)\n",
    "# ==============================================================================\n",
    "print(\"Loading dataset pelatihan untuk membangun prototipe...\")\n",
    "\n",
    "# --- Datasets & Dataloaders (Hanya untuk train set) ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Dapatkan nama kelas\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Ditemukan {len(class_names)} kelas: {class_names}\")\n",
    "\n",
    "# --- Ekstraksi embedding pelatihan ---\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_train.append(emb.cpu().numpy())\n",
    "        y_train.append(labels.numpy())\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "print(f\"Ekstraksi {len(X_train)} embedding pelatihan selesai.\")\n",
    "\n",
    "# --- Membangun prototipe kelas (dari skrip Anda) ---\n",
    "prototypes = {}\n",
    "for idx, cls in enumerate(train_dataset.classes):  # use ImageFolder.classes\n",
    "    class_idx = train_dataset.class_to_idx[cls]\n",
    "    class_embeddings = X_train[y_train == class_idx]\n",
    "    \n",
    "    if len(class_embeddings) > 0:\n",
    "        prototypes[class_idx] = class_embeddings.mean(axis=0)\n",
    "        prototypes[class_idx] /= np.linalg.norm(prototypes[class_idx])\n",
    "    else:\n",
    "        print(f\"Peringatan: Tidak ada sampel ditemukan untuk kelas {cls} (idx: {class_idx})\")\n",
    "\n",
    "print(f\"Prototipe untuk {len(prototypes)} kelas berhasil dibuat.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 2: FUNGSI HELPER\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Fungsi Prediksi (dari skrip Anda) ---\n",
    "def predict_embedding(embedding, prototypes, threshold=0.8):\n",
    "    if embedding.ndim == 0 or np.linalg.norm(embedding) == 0:\n",
    "        return -1, 0.0 # Embedding tidak valid\n",
    "    \n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    best_cls = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for cls_idx, proto in prototypes.items():\n",
    "        sim = np.dot(embedding, proto)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_cls = cls_idx\n",
    "            \n",
    "    if best_sim < threshold:\n",
    "        return -1, best_sim  # Unknown\n",
    "        \n",
    "    return best_cls, best_sim\n",
    "\n",
    "# --- Fungsi Pencerah Gambar ---\n",
    "def auto_adjust_brightness(frame, target_brightness=90, low_threshold=70):\n",
    "    \"\"\"Mencerahkan frame jika brightness rata-rata di bawah ambang batas.\"\"\"\n",
    "    \n",
    "    # Konversi ke Grayscale untuk mengecek brightness\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    \n",
    "    if mean_brightness < low_threshold:\n",
    "        # Hitung seberapa banyak brightness perlu ditambah\n",
    "        delta = target_brightness - mean_brightness\n",
    "        \n",
    "        # Tambah brightness menggunakan cv2.convertScaleAbs untuk penanganan saturasi\n",
    "        # alpha=1 (kontras tetap), beta=delta (brightness ditambah)\n",
    "        bright_frame = cv2.convertScaleAbs(frame, alpha=1.0, beta=delta)\n",
    "        return bright_frame\n",
    "        \n",
    "    return frame\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 3: REAL-TIME LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Inisialisasi MTCNN ---\n",
    "mtcnn = MTCNN(\n",
    "    keep_all=True,              # Deteksi semua wajah\n",
    "    post_process=False,         # Jangan lakukan normalisasi (kita lakukan di transform)\n",
    "    min_face_size=40,           # Abaikan wajah yang sangat kecil\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Inisialisasi Video Capture ---\n",
    "# Ganti URL ini dengan URL dari aplikasi IP Webcam Anda\n",
    "# Contoh: \"http://192.168.1.10:8080/video\"\n",
    "# video_url = \"http://YOUR_PHONE_IP:8080/video\"\n",
    "\n",
    "# Untuk testing menggunakan Webcam internal, ganti 'video_url' dengan 0\n",
    "# cap = cv2.VideoCapture(0) \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Tidak bisa membuka stream video.\")\n",
    "    print(f\"Pastikan URL '{video_url}' benar dan aplikasi IP Webcam berjalan.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Memulai stream video... Tekan 'q' untuk keluar.\")\n",
    "\n",
    "# Variabel untuk FPS\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Gagal membaca frame, mencoba menghubungkan kembali...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(video_url)\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    # 1. Pencerahan Otomatis (Sesuai permintaan Anda)\n",
    "    frame = auto_adjust_brightness(frame)\n",
    "\n",
    "    # 2. Konversi BGR (OpenCV) ke RGB (PIL) untuk MTCNN\n",
    "    # MTCNN mengharapkan input PIL Image\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 3. Deteksi Wajah dengan MTCNN\n",
    "    boxes, _, landmarks = mtcnn.detect(img_pil, landmarks=True)\n",
    "\n",
    "    # 4. Loop untuk setiap wajah yang terdeteksi\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):  # 'i' ditambahkan di sini\n",
    "            x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "            # Pastikan koordinat valid (tidak keluar dari frame)\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(frame.shape[1] - 1, x2)\n",
    "            y2 = min(frame.shape[0] - 1, y2)\n",
    "\n",
    "            # Pastikan crop valid\n",
    "            if y2 <= y1 or x2 <= x1:\n",
    "                continue\n",
    "\n",
    "            # 5. Crop wajah dari frame BGR asli\n",
    "            face_crop_bgr = frame[y1:y2, x1:x2]\n",
    "\n",
    "\n",
    "            # 5.5 ALIGN Wajah menggunakan Landmark Mata\n",
    "            try:\n",
    "                # Ambil landmark untuk wajah ini\n",
    "                current_landmarks = landmarks[i]\n",
    "                left_eye = current_landmarks[0]   # [x, y] mata kiri\n",
    "                right_eye = current_landmarks[1]  # [x, y] mata kanan\n",
    "                \n",
    "                # Hitung sudut antara dua mata\n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                # Hitung sudut dalam derajat\n",
    "                angle_deg = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "                # Dapatkan center dari face crop (untuk titik rotasi)\n",
    "                (h_crop, w_crop) = face_crop_bgr.shape[:2]\n",
    "                center_crop = (w_crop // 2, h_crop // 2)\n",
    "\n",
    "                # Dapatkan matriks rotasi\n",
    "                M = cv2.getRotationMatrix2D(center_crop, angle_deg, 1.0)\n",
    "                \n",
    "                # Terapkan rotasi\n",
    "                # cv2.INTER_CUBIC memberikan kualitas lebih baik\n",
    "                # borderMode=cv2.BORDER_CONSTANT mengisi area hitam jika ada\n",
    "                aligned_face_crop = cv2.warpAffine(face_crop_bgr, M, (w_crop, h_crop), \n",
    "                                                flags=cv2.INTER_CUBIC, \n",
    "                                                borderMode=cv2.BORDER_CONSTANT,\n",
    "                                                borderValue=(0, 0, 0))\n",
    "                \n",
    "                # Gunakan wajah yang sudah di-align\n",
    "                face_to_process = aligned_face_crop\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Jika gagal (misal landmark tidak terdeteksi), pakai crop asli\n",
    "                face_to_process = face_crop_bgr\n",
    "\n",
    "            # 6. Preprocessing wajah untuk model\n",
    "            # Konversi crop (BGR) ke PIL (RGB)\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Terapkan transformasi\n",
    "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # 7. Dapatkan Embedding\n",
    "            with torch.no_grad():\n",
    "                emb = model(face_tensor)\n",
    "            \n",
    "            emb = emb.cpu().numpy().squeeze()\n",
    "\n",
    "            # 8. Prediksi menggunakan Prototipe\n",
    "            cls_idx, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "\n",
    "            # 9. Dapatkan Label Nama\n",
    "            if cls_idx == -1:\n",
    "                label = \"Unknown\"\n",
    "                color = (0, 0, 255) # Merah\n",
    "            else:\n",
    "                label = class_names[cls_idx]\n",
    "                color = (0, 255, 0) # Hijau\n",
    "            \n",
    "            display_text = f\"{label} ({sim*100:.1f}%)\"\n",
    "\n",
    "            # 10. Gambar Bounding Box dan Label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # Hitung dan tampilkan FPS\n",
    "    frame_count += 1\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    if elapsed >= 1.0:\n",
    "        fps = frame_count / elapsed\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow('Real-time Face Recognition (Tekan q untuk keluar)', frame)\n",
    "\n",
    "    # Tombol Keluar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Stream dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "Loading class names from dataset directory...\n",
      "Ditemukan 54 kelas: ['5221911012_Debora', '5221911025_Anggun', '5231811002_MichaelAndrewDeHaan', '5231811004_Hamdanu Fahmi Utomo', '5231811005_Akhmad Nabil Saputra', '5231811006_Daniel Granesa Kiara', '5231811007_Amalia Dwi Ramadhani', '5231811008_Sophia', '5231811009_Otniel Chresto Purwandi', '5231811010_MEYLAN ARYANI', '5231811013_KusumaRatih', '5231811014_Dian Eka Pratiwi', '5231811015_Fadilah Ratu Azzahra', '5231811016_Kesya', '5231811017_Maulana Ahmad Muhaimin', '5231811018_Sulis Septiani Putri', '5231811019_Chronika', '5231811021_NASHA SHINTA ABP', '5231811022_Lathif Ramadhan', '5231811023_Rahma Fieka Januarni', '5231811024_Maria Febronia Boa', '5231811025_Novera', '5231811026_ULFAH NAFIAH', '5231811027_Naufal', \"5231811028_Ma'ruf Ndaru Sasono\", '5231811029_Andini', '5231811030_Antonia Idan Huler', '5231811031_CindyKusumaningrum', '5231811033_Rama Panji Nararendra Cahaya', '5231811034_Shilsylia Putri Devitasary', '5231811035_Yogi Hanusanjaya', '5231811036_Giffari Riyanda Pradithya', '5231811037_Muhammad Ibra Ramadhon', '5231811038_yudit manda', '5231811039_Rambang Widyadana', '5231911001_Farma', '5231911002_Desak Gde Devika Anantha Armi', '5231911003_ulen', '5231911004_al faisal selan', '5231911005_Deraya', '5231911006_ARI DHARMA DEVANANTA', '5231911007_julia eliza w', '5231911008_Faren', '5231911009_Ica Nurcahya', '5231911010_Jeny Amelindrika Putri', '5231911011_Nirmala Fitri Margitaya', '5231911012_Thalita Revalyna Maharani', '5231911013_REVA SEBRINA SALSABILA', '5231911014_Asti Eka Rahayu', '5231911016_Ameliawati', '5231911017_RAFI AUFA MIRZA', '5231911018_FIRMAN', '5231911019_Glory Valentio Duska Putra', '5231911020_siti nabila maulidya']\n",
      "Loading model weights from: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\best_mobilenetv2_face.pth\n",
      "Model classifier berhasil dimuat.\n",
      "Memulai stream video... Tekan 'q' untuk keluar.\n",
      "Stream dihentikan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import mobilenet_v2 # <-- IMPORT BARU\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# --- Variabel & Path ---\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "# Ganti dengan path model classifier Anda\n",
    "model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\best_mobilenetv2_face.pth\" \n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 1: MEMUAT MODEL CLASSIFIER DAN NAMA KELAS\n",
    "# ==============================================================================\n",
    "print(\"Loading class names from dataset directory...\")\n",
    "# Kita perlu dataset HANYA untuk mendapatkan nama kelas\n",
    "temp_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_names = temp_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "del temp_dataset # Hapus, tidak dipakai lagi\n",
    "\n",
    "print(f\"Ditemukan {num_classes} kelas: {class_names}\")\n",
    "\n",
    "# --- Load model pre-trained (Sesuai permintaan Anda) ---\n",
    "model = mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "num_features = model.classifier[1].in_features\n",
    "\n",
    "# --- Ganti classifier terakhir ---\n",
    "model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Load state dictionary Anda ---\n",
    "print(f\"Loading model weights from: {model_path}\")\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Model classifier berhasil dimuat.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 2: TRANSFORMASI\n",
    "# ==============================================================================\n",
    "# Transformasi untuk model (sama seperti sebelumnya)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 3: FUNGSI HELPER (Hanya Pencerah)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Fungsi Pencerah Gambar ---\n",
    "def auto_adjust_brightness(frame, target_brightness=90, low_threshold=70):\n",
    "    \"\"\"Mencerahkan frame jika brightness rata-rata di bawah ambang batas.\"\"\"\n",
    "    \n",
    "    # Konversi ke Grayscale untuk mengecek brightness\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    \n",
    "    if mean_brightness < low_threshold:\n",
    "        # Hitung seberapa banyak brightness perlu ditambah\n",
    "        delta = target_brightness - mean_brightness\n",
    "        \n",
    "        # Tambah brightness menggunakan cv2.convertScaleAbs\n",
    "        bright_frame = cv2.convertScaleAbs(frame, alpha=1.0, beta=delta)\n",
    "        return bright_frame\n",
    "        \n",
    "    return frame\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 4: REAL-TIME LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Inisialisasi MTCNN ---\n",
    "mtcnn = MTCNN(\n",
    "    keep_all=True,              # Deteksi semua wajah\n",
    "    post_process=False,         # Jangan lakukan normalisasi\n",
    "    min_face_size=40,           # Abaikan wajah yang sangat kecil\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Inisialisasi Video Capture ---\n",
    "cap = cv2.VideoCapture(0) # Gunakan webcam internal\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Tidak bisa membuka webcam (index 0).\")\n",
    "    exit()\n",
    "\n",
    "print(\"Memulai stream video... Tekan 'q' untuk keluar.\")\n",
    "\n",
    "# Variabel untuk FPS\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "# Tentukan threshold kepercayaan (confidence)\n",
    "confidence_threshold = 0.2 # 75% - Sesuaikan jika perlu\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Gagal membaca frame.\")\n",
    "        break\n",
    "\n",
    "    # 1. Pencerahan Otomatis (Sesuai permintaan Anda)\n",
    "    frame_bright = auto_adjust_brightness(frame)\n",
    "\n",
    "    # 2. Konversi BGR (OpenCV) ke RGB (PIL) untuk MTCNN\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame_bright, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 3. Deteksi Wajah dengan MTCNN (dan dapatkan landmarks)\n",
    "    boxes, _, landmarks = mtcnn.detect(img_pil, landmarks=True)\n",
    "\n",
    "    # 4. Loop untuk setiap wajah yang terdeteksi\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "            # Sanitasi koordinat\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(frame.shape[1] - 1, x2)\n",
    "            y2 = min(frame.shape[0] - 1, y2)\n",
    "\n",
    "            if y2 <= y1 or x2 <= x1:\n",
    "                continue\n",
    "\n",
    "            # 5. Crop wajah dari frame BGR asli\n",
    "            face_crop_bgr = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # 5.5 ALIGN Wajah menggunakan Landmark Mata\n",
    "            try:\n",
    "                current_landmarks = landmarks[i]\n",
    "                left_eye = current_landmarks[0]\n",
    "                right_eye = current_landmarks[1]\n",
    "                \n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                angle_deg = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "                (h_crop, w_crop) = face_crop_bgr.shape[:2]\n",
    "                center_crop = (w_crop // 2, h_crop // 2)\n",
    "\n",
    "                M = cv2.getRotationMatrix2D(center_crop, angle_deg, 1.0)\n",
    "                \n",
    "                aligned_face_crop = cv2.warpAffine(face_crop_bgr, M, (w_crop, h_crop), \n",
    "                                                   flags=cv2.INTER_CUBIC, \n",
    "                                                   borderMode=cv2.BORDER_CONSTANT)\n",
    "                \n",
    "                face_to_process = aligned_face_crop\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Jika gagal align, pakai crop asli\n",
    "                face_to_process = face_crop_bgr\n",
    "            \n",
    "            # 6. Preprocessing wajah untuk model\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_to_process, cv2.COLOR_BGR2RGB))\n",
    "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # --- â–¼â–¼â–¼ LOGIKA PREDIKSI BARU â–¼â–¼â–¼ ---\n",
    "            # 7. Dapatkan Prediksi (Logits)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(face_tensor) # Output adalah [1, num_classes]\n",
    "                \n",
    "                # 8. Ubah Logits ke Probabilitas (Softmax)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # 9. Dapatkan kelas dan kepercayaan (confidence) tertinggi\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                cls_idx = predicted_idx.item()\n",
    "                conf = confidence.item()\n",
    "\n",
    "            # 10. Dapatkan Label Nama berdasarkan threshold\n",
    "            if conf < confidence_threshold:\n",
    "                label = \"Unknown\"\n",
    "                color = (0, 0, 255) # Merah\n",
    "            else:\n",
    "                label = class_names[cls_idx]\n",
    "                color = (0, 255, 0) # Hijau\n",
    "            \n",
    "            display_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "            # --- â–²â–²â–² AKHIR LOGIKA PREDIKSI BARU â–²â–²â–² ---\n",
    "\n",
    "            # 11. Gambar Bounding Box dan Label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # Hitung dan tampilkan FPS\n",
    "    frame_count += 1\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    if elapsed >= 1.0:\n",
    "        fps = frame_count / elapsed\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow('Real-time Face Recognition (Classifier) - Tekan q', frame)\n",
    "\n",
    "    # Tombol Keluar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Stream dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ecad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean intra-class sim: 0.9285825\n",
      "Mean inter-class sim: 0.0108167315\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Example: check intra-class vs inter-class similarities\n",
    "prototypes_norm = {k: v/np.linalg.norm(v) for k,v in prototypes.items()}\n",
    "\n",
    "intra_sims = []\n",
    "inter_sims = []\n",
    "\n",
    "for cls_idx, proto in prototypes_norm.items():\n",
    "    emb_cls = X_train[y_train == cls_idx]\n",
    "    emb_cls = emb_cls / np.linalg.norm(emb_cls, axis=1, keepdims=True)\n",
    "    # intra-class\n",
    "    for i,j in combinations(range(len(emb_cls)), 2):\n",
    "        intra_sims.append(np.dot(emb_cls[i], emb_cls[j]))\n",
    "    # inter-class\n",
    "    for other_idx, other_proto in prototypes_norm.items():\n",
    "        if other_idx != cls_idx:\n",
    "            inter_sims.append(np.dot(proto, other_proto))\n",
    "\n",
    "print(\"Mean intra-class sim:\", np.mean(intra_sims))\n",
    "print(\"Mean inter-class sim:\", np.mean(inter_sims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583c55a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m image = preprocess_image(image, brightness_thresh=\u001b[32m0.5\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Apply same preprocessing as your train/test set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m img_tensor = \u001b[43mtest_transform\u001b[49m(image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Extract embedding\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'test_transform' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Folder path ---\n",
    "# img_folder = r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\\WhatsApp Image 2025_\"\n",
    "img_folder = r\"D:\\download_d\\data_test\"\n",
    "\n",
    "# --- Helper function: conditional preprocessing ---\n",
    "def preprocess_image(image_pil, brightness_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Convert to RGB PIL image -> possibly brighten if too dark -> return PIL image.\n",
    "    \"\"\"\n",
    "    # Convert PIL to OpenCV (numpy array)\n",
    "    img = np.array(image_pil)  # shape H,W,3, RGB\n",
    "    img_cv = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Convert to float [0,1]\n",
    "    img_float = img_cv.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Compute brightness (average luminance)\n",
    "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "    brightness = np.mean(gray) / 255.0\n",
    "    \n",
    "    # If too dark, adjust brightness & contrast\n",
    "    if brightness < brightness_thresh:\n",
    "        alpha = 1.2  # contrast\n",
    "        beta = 0.1   # brightness\n",
    "        img_float = np.clip(alpha * img_float + beta, 0, 1)\n",
    "        # Optional gamma correction\n",
    "        gamma = 1.2\n",
    "        img_float = np.power(img_float, 1/gamma)\n",
    "    \n",
    "    # Convert back to uint8 and PIL\n",
    "    img_float = (img_float * 255).astype(np.uint8)\n",
    "    img_rgb = cv2.cvtColor(img_float, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    \n",
    "    return img_pil\n",
    "\n",
    "# Get all image files\n",
    "img_paths = [os.path.join(img_folder, f) \n",
    "             for f in os.listdir(img_folder) \n",
    "             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Process all images\n",
    "for img_path in img_paths:\n",
    "    # Load image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    # Conditional preprocessing\n",
    "    image = preprocess_image(image, brightness_thresh=0.5)\n",
    "    \n",
    "    # Apply same preprocessing as your train/test set\n",
    "    img_tensor = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Extract embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()[0]  # shape (128,)\n",
    "\n",
    "    # Predict using prototypes\n",
    "    predicted_cls, sim = predict_embedding(embedding, prototypes, threshold=0.6)  # adjust threshold\n",
    "    if predicted_cls == -1:\n",
    "        predicted_name = \"Unknown\"\n",
    "    else:\n",
    "        predicted_name = train_dataset.classes[predicted_cls]\n",
    "\n",
    "    print(f\"Image: {img_path} | Predicted: {predicted_name} | Cosine sim: {sim:.3f}\")\n",
    "\n",
    "    # Display image with label\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted: {predicted_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "cropper = FaceCropper()\n",
    "cropper.process_folder(r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\", r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ab1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_3256\\1998673123.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ArcFaceLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 195\u001b[39m\n\u001b[32m    192\u001b[39m model.load_state_dict(torch.load(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mKuliah\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDeep Learning\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtugas_4\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMobileFace_Net\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage))\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# --- Loss & Optimizer ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m criterion = \u001b[43mArcFaceLoss\u001b[49m(\n\u001b[32m    196\u001b[39m     num_classes=\u001b[32m54\u001b[39m,\n\u001b[32m    197\u001b[39m     embedding_size=\u001b[32m512\u001b[39m,\n\u001b[32m    198\u001b[39m     margin=\u001b[32m0.5\u001b[39m,   \u001b[38;5;66;03m# standard margin; increases angular separation\u001b[39;00m\n\u001b[32m    199\u001b[39m     scale=\u001b[32m30.0\u001b[39m    \u001b[38;5;66;03m# keep the same\u001b[39;00m\n\u001b[32m    200\u001b[39m ).to(device)\n\u001b[32m    203\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.0005598380474533065\u001b[39m, weight_decay=\u001b[32m0.0005501701817196964\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# optimizer = torch.optim.SGD(model.parameters(), \u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m#                             lr=0.1, \u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m#                             momentum=0.9, \u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m#                             weight_decay=5e-4)\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# --- Training Loop ---\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ArcFaceLoss' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, 512, bias=False)\n",
    "        self.bn = BatchNorm1d(512)\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Input:\", x.shape)\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        # print(\"After conv1:\", out.shape)\n",
    "        \n",
    "        out = self.conv2_dw(out)\n",
    "        # print(\"After conv2_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "        # print(\"After conv_23:\", out.shape)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        # print(\"After conv_3:\", out.shape)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "        # print(\"After conv_34:\", out.shape)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        # print(\"After conv_4:\", out.shape)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "        # print(\"After conv_45:\", out.shape)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "        # print(\"After conv_5:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "        # print(\"After conv_6_sep:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "        # print(\"After conv_6_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "        # print(\"After conv_6_flatten:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        # print(\"After linear:\", out.shape)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        # print(\"After batchnorm:\", out.shape)\n",
    "\n",
    "        out = l2_norm(out)\n",
    "        # print(\"After l2_norm:\", out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "## INI NYOBA MENAMBAH PAKE LAIN\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=512,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10*LR_MIN_EXP, 10*LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10*WD_MIN_EXP, 10*WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f6948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6eb38330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_13204\\2550465223.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0989e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_3256\\2466821108.py:298: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
      "Epoch 1/50 - Training:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [00:34<00:32,  1.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 336\u001b[39m\n\u001b[32m    334\u001b[39m model.train()\n\u001b[32m    335\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m - Training\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1404\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "    \n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, 512, bias=False)\n",
    "        self.bn = BatchNorm1d(512)\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Input:\", x.shape)\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        # print(\"After conv1:\", out.shape)\n",
    "        \n",
    "        out = self.conv2_dw(out)\n",
    "        # print(\"After conv2_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "        # print(\"After conv_23:\", out.shape)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        # print(\"After conv_3:\", out.shape)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "        # print(\"After conv_34:\", out.shape)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        # print(\"After conv_4:\", out.shape)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "        # print(\"After conv_45:\", out.shape)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "        # print(\"After conv_5:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "        # print(\"After conv_6_sep:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "        # print(\"After conv_6_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "        # print(\"After conv_6_flatten:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        # print(\"After linear:\", out.shape)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        # print(\"After batchnorm:\", out.shape)\n",
    "\n",
    "        out = l2_norm(out)\n",
    "        # print(\"After l2_norm:\", out.shape)\n",
    "\n",
    "        return out\n",
    "\n",
    "## INI NYOBA MENAMBAH PAKE LAIN\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=512,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10*LR_MIN_EXP, 10*LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10*WD_MIN_EXP, 10*WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_bestss.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347a7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "    \n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class MobileFaceNet(Module):\n",
    "    def ___init___(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, 512, bias=False)\n",
    "        self.bn = BatchNorm1d(512)\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Input:\", x.shape)\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        # print(\"After conv1:\", out.shape)\n",
    "        \n",
    "        out = self.conv2_dw(out)\n",
    "        # print(\"After conv2_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "        # print(\"After conv_23:\", out.shape)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        # print(\"After conv_3:\", out.shape)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "        # print(\"After conv_34:\", out.shape)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        # print(\"After conv_4:\", out.shape)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "        # print(\"After conv_45:\", out.shape)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "        # print(\"After conv_5:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "        # print(\"After conv_6_sep:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "        # print(\"After conv_6_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "        # print(\"After conv_6_flatten:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        # print(\"After linear:\", out.shape)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        # print(\"After batchnorm:\", out.shape)\n",
    "\n",
    "        out = l2_norm(out)\n",
    "        # print(\"After l2_norm:\", out.shape)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6137dd7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MobileFaceNet.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28minput\u001b[39m = torch.Tensor(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m112\u001b[39m, \u001b[32m112\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m net = \u001b[43mMobileFaceNet\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      4\u001b[39m x = net(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(x.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:485\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    480\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    481\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m     )\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    486\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[33;03mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[33;03mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[33;03msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    496\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__setattr__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: MobileFaceNet.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.Tensor(2, 3, 112, 112).to(device)\n",
    "net = MobileFaceNet(512).to(device)\n",
    "x = net(input)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "32833b5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MobileFaceNet.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m test_loader = DataLoader(test_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m4\u001b[39m, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# --- Model ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m model = \u001b[43mMobileFaceNet\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     52\u001b[39m model.load_state_dict(torch.load(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mD:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mKuliah\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDeep Learning\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtugas_4\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMobileFace_Net\u001b[39m\u001b[33m\"\u001b[39m, map_location=\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage))\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Loss & Optimizer ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:485\u001b[39m, in \u001b[36mModule.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    480\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() got an unexpected keyword argument \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    481\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    482\u001b[39m     )\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    486\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m were\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m given\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    488\u001b[39m     )\n\u001b[32m    490\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[33;03mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[33;03mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[33;03msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[32m    495\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    496\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__setattr__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: MobileFaceNet.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## INI NYOBA MENAMBAH PAKE LAIN\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=512,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10*LR_MIN_EXP, 10*LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10*WD_MIN_EXP, 10*WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_bestddd.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1cc73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
