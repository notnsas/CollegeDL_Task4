{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59bce78c-accd-4577-8207-3071c2446617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights, mobilenet_v3_large, MobileNet_V3_Large_Weights, mobilenet_v2, MobileNet_V2_Weights\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import tqdm\n",
    "import random \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import cv2\n",
    "import imageio\n",
    "from facenet_pytorch import MTCNN\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from tqdm import tqdm\n",
    "from facenet_pytorch import MTCNN\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "# Import datasets class buat dataloader\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "# from utils import DataPreprocessing, TripletDataset, DatasetClassification, class_to_image_path, extract_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e97f8da2-77ee-4175-9f28-50a5a580ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import math\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "class FaceCropper:\n",
    "    def __init__(self, mtcnn_device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.mtcnn = MTCNN(keep_all=False, device=mtcnn_device)\n",
    "        self.yolo_fallback = YOLO(\n",
    "            r\"D:\\Models\\huggingface\\hub\\models--arnabdhar--YOLOv8-Face-Detection\\snapshots\\52fa54977207fa4f021de949b515fb19dcab4488\\model.pt\"\n",
    "        )\n",
    "\n",
    "    def extract_class(self, txt):\n",
    "        raw_class = re.split(r\"[-_.]\", txt, 2)[0:2]\n",
    "        complete_class = raw_class[0] + \"_\" + raw_class[1]\n",
    "        complete_class = re.sub(r'[\\d\\s]+$', '', complete_class)\n",
    "        return complete_class\n",
    "    \n",
    "    def auto_rotate_image(self, image):\n",
    "        \"\"\"Rotate image using MTCNN landmarks\"\"\"\n",
    "        _, _, landmarks = self.mtcnn.detect(image, landmarks=True)\n",
    "        if landmarks is not None:\n",
    "            left_eye, right_eye = landmarks[0][:2]\n",
    "            dx, dy = right_eye[0] - left_eye[0], right_eye[1] - left_eye[1]\n",
    "            angle = math.degrees(math.atan2(dy, dx))\n",
    "            image = image.rotate(-angle, expand=True)\n",
    "        return image\n",
    "    \n",
    "    def crop_face(self, image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            image = ImageOps.exif_transpose(image)\n",
    "            image = self.auto_rotate_image(image)\n",
    "    \n",
    "            # First try MTCNN\n",
    "            boxes, _ = self.mtcnn.detect(image)\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                box = boxes[0]\n",
    "            else:\n",
    "                # Fall back to YOLO\n",
    "                results = self.yolo_fallback(image_path)\n",
    "                if results and len(results[0].boxes.xyxy) > 0:\n",
    "                    box = results[0].boxes.xyxy[0].cpu().numpy()\n",
    "                else:\n",
    "                    return None\n",
    "    \n",
    "            left, top, right, bottom = map(int, box)\n",
    "            return image.crop((left, top, right, bottom))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def process_folder(self, input_dir, output_dir, batch_size=32, max_workers=8):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        image_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_files = image_files[i:i + batch_size]\n",
    "        \n",
    "            def process_file(fname):\n",
    "                path = os.path.join(input_dir, fname)\n",
    "                cropped = self.crop_face(path)\n",
    "                if cropped:\n",
    "                    class_name = self.extract_class(fname)\n",
    "                    class_dir = os.path.join(output_dir, class_name)\n",
    "                    os.makedirs(class_dir, exist_ok=True)  # Create class subfolder safely\n",
    "                    save_path = os.path.join(class_dir, fname)  # keep original filename\n",
    "                    # If file exists, append counter\n",
    "                    counter = 1\n",
    "                    base, ext = os.path.splitext(save_path)\n",
    "                    while os.path.exists(save_path):\n",
    "                        save_path = f\"{base}_{counter}{ext}\"\n",
    "                        counter += 1\n",
    "                    cropped.save(save_path)\n",
    "        \n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                list(tqdm(executor.map(process_file, batch_files), total=len(batch_files)))\n",
    "        \n",
    "            # Clean memory\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68feb7a-e0b8-4662-9421-5304bc30e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Starting TRAIN image cropping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.72it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:47<00:00,  1.50s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.17it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.86it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:33<00:00,  4.79s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.66it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:41<00:00,  1.31s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.74it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:05<00:00,  5.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.69it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:15<00:00,  2.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:01<00:00,  1.91s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:03<00:00,  3.87s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:31<00:00,  1.00it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:10<00:00,  3.05it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:09<00:00,  3.42it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:31<00:00,  1.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:43<00:00,  5.10s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [05:39<00:00, 10.59s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:33<00:00,  1.04s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.85it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.97it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:31<00:00,  1.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:16<00:00,  1.92it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:22<00:00,  1.41it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:24<00:00,  1.30it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:16<00:00,  1.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:20<00:00,  1.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:11<00:00,  2.77it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:43<00:00,  1.36s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:37<00:00,  1.17s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:35<00:00,  1.11s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:38<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:07<00:00,  4.24it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  7.31it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:02<00:00,  6.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ£ Starting TEST image cropping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:08<00:00,  3.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:28<00:00,  1.13it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:15<00:00,  2.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:31<00:00,  1.01it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:37<00:00,  1.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:18<00:00,  1.68it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:59<00:00,  1.85s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.55it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:07<00:00,  4.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:17<00:00,  1.79it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [03:09<00:00,  5.91s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:22<00:00,  1.43it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:09<00:00,  3.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:27<00:00,  1.15it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [05:36<00:00, 10.51s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:09<00:00,  3.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:38<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.20it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:33<00:00,  1.06s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:12<00:00,  2.57it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.18it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:34<00:00,  1.07s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:16<00:00,  1.94it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:38<00:00,  1.20s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [01:24<00:00,  2.63s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:09<00:00,  3.45it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All done! Cropped images saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize cropper\n",
    "cropper = FaceCropper()\n",
    "\n",
    "# --- Train paths ---\n",
    "train_input = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train\"\n",
    "train_output = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "\n",
    "print(\"ðŸŸ¢ Starting TRAIN image cropping...\")\n",
    "cropper.process_folder(train_input, train_output)\n",
    "\n",
    "# --- Test paths ---\n",
    "test_input = train_input.replace(\"Train\", \"Test\")\n",
    "test_output = train_output.replace(\"Train\", \"Test\")\n",
    "\n",
    "print(\"\\nðŸŸ£ Starting TEST image cropping...\")\n",
    "cropper.process_folder(test_input, test_output)\n",
    "\n",
    "print(\"\\nâœ… All done! Cropped images saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c700a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe cropper\n",
    "cropper = FaceCropperMP()\n",
    "\n",
    "# --- Train paths ---\n",
    "train_input = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train\"\n",
    "train_output = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MP\"\n",
    "\n",
    "print(\"ðŸŸ¢ Starting TRAIN image cropping...\")\n",
    "cropper.process_folder(train_input, train_output)\n",
    "\n",
    "# --- Test paths ---\n",
    "test_input = train_input.replace(\"Train\", \"Test\")\n",
    "test_output = train_output.replace(\"Train\", \"Test\")\n",
    "\n",
    "print(\"\\nðŸŸ£ Starting TEST image cropping...\")\n",
    "cropper.process_folder(test_input, test_output)\n",
    "\n",
    "print(\"\\nâœ… All done! Cropped images saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e10a293-26b6-42a0-af3c-9a35fd95c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Paths\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "# test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "\n",
    "# Transformations\n",
    "# MobileNet usually expects 224x224 input and normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# Dataloaders (fast with multiple workers)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ed557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import math\n",
    "# from PIL import Image, ImageOps\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import mediapipe as mp\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# class FaceCropperMP:\n",
    "#     def __init__(self):\n",
    "#         self.face_mesh = mp_face_mesh.FaceMesh(\n",
    "#             static_image_mode=True,\n",
    "#             max_num_faces=1,\n",
    "#             refine_landmarks=True,\n",
    "#             min_detection_confidence=0.5\n",
    "#         )\n",
    "\n",
    "#     def extract_class(self, filename):\n",
    "#         parts = filename.split('_')[:2]\n",
    "#         class_name = '_'.join(parts)\n",
    "#         return ''.join([c for c in class_name if not c.isdigit()]).strip()\n",
    "\n",
    "#     def auto_rotate_and_crop(self, image):\n",
    "#         try:\n",
    "#             img_rgb = image.convert(\"RGB\")\n",
    "#             results = self.face_mesh.process(np.array(img_rgb))\n",
    "\n",
    "#             if not results.multi_face_landmarks:\n",
    "#                 return None\n",
    "\n",
    "#             landmarks = results.multi_face_landmarks[0].landmark\n",
    "#             # approximate eyes\n",
    "#             left_eye = landmarks[33]\n",
    "#             right_eye = landmarks[263]\n",
    "\n",
    "#             width, height = image.size\n",
    "#             lx, ly = int(left_eye.x * width), int(left_eye.y * height)\n",
    "#             rx, ry = int(right_eye.x * width), int(right_eye.y * height)\n",
    "\n",
    "#             # rotation angle\n",
    "#             dx, dy = rx - lx, ry - ly\n",
    "#             angle = math.degrees(math.atan2(dy, dx))\n",
    "#             image = image.rotate(-angle, expand=True)\n",
    "\n",
    "#             # crop bounding box around landmarks\n",
    "#             xs = [int(lm.x * width) for lm in landmarks]\n",
    "#             ys = [int(lm.y * height) for lm in landmarks]\n",
    "#             left, top, right, bottom = min(xs), min(ys), max(xs), max(ys)\n",
    "\n",
    "#             return image.crop((left, top, right, bottom))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to process image: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def process_folder(self, input_dir, output_dir, batch_size=32, max_workers=8):\n",
    "#         if not os.path.exists(output_dir):\n",
    "#             os.makedirs(output_dir)\n",
    "\n",
    "#         image_files = [f for f in os.listdir(input_dir)\n",
    "#                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "#         for i in range(0, len(image_files), batch_size):\n",
    "#             batch_files = image_files[i:i + batch_size]\n",
    "\n",
    "#             def process_file(fname):\n",
    "#                 path = os.path.join(input_dir, fname)\n",
    "#                 try:\n",
    "#                     image = Image.open(path)\n",
    "#                     image = ImageOps.exif_transpose(image)\n",
    "#                     cropped = self.auto_rotate_and_crop(image)\n",
    "#                     if cropped:\n",
    "#                         class_name = self.extract_class(fname)\n",
    "#                         class_dir = os.path.join(output_dir, class_name)\n",
    "#                         os.makedirs(class_dir, exist_ok=True)\n",
    "#                         save_path = os.path.join(class_dir, fname)\n",
    "#                         counter = 1\n",
    "#                         base, ext = os.path.splitext(save_path)\n",
    "#                         while os.path.exists(save_path):\n",
    "#                             save_path = f\"{base}_{counter}{ext}\"\n",
    "#                             counter += 1\n",
    "#                         cropped.save(save_path)\n",
    "#                     else:\n",
    "#                         print(f\"Skipped: {fname}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed: {fname}, {e}\")\n",
    "\n",
    "#             with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#                 list(tqdm(executor.map(process_file, batch_files), total=len(batch_files)))\n",
    "\n",
    "#             gc.collect()\n",
    "\n",
    "# # Example usage\n",
    "# # cropper = FaceCropperMP()\n",
    "# # cropper.process_folder(\"train_images\", \"train_cropped\")\n",
    "# # cropper.process_folder(\"test_images\", \"test_cropped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956a95ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size, margin, scale):\n",
    "        \"\"\"\n",
    "        ArcFace: Additive Angular Margin Loss for Deep Face Recognition\n",
    "        (https://arxiv.org/pdf/1801.07698.pdf)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "            margin: m in the paper, the angular margin penalty in radians\n",
    "            scale: s in the paper, feature scale\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        cosine = self.get_cosine(embeddings) # (None, n_classes)\n",
    "        mask = self.get_target_mask(labels) # (None, n_classes)\n",
    "        cosine_of_target_classes = cosine[mask == 1] # (None, )\n",
    "        modified_cosine_of_target_classes = self.modify_cosine_of_target_classes(\n",
    "            cosine_of_target_classes\n",
    "        ) # (None, )\n",
    "        diff = (modified_cosine_of_target_classes - cosine_of_target_classes).unsqueeze(1) # (None,1)\n",
    "        logits = cosine + (mask * diff) # (None, n_classes)\n",
    "        logits = self.scale_logits(logits) # (None, n_classes)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)\n",
    "        \n",
    "    def get_cosine(self, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "        Returns:\n",
    "            cosine: (None, n_classes)\n",
    "        \"\"\"\n",
    "        cosine = F.linear(F.normalize(embeddings), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "    def get_target_mask(self, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            mask: (None, n_classes)\n",
    "        \"\"\"\n",
    "        batch_size = labels.size(0)\n",
    "        onehot = torch.zeros(batch_size, self.num_classes, device=labels.device)\n",
    "        onehot.scatter_(1, labels.unsqueeze(-1), 1)\n",
    "        return onehot\n",
    "        \n",
    "    def modify_cosine_of_target_classes(self, cosine_of_target_classes):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cosine_of_target_classes: (None,)\n",
    "        Returns:\n",
    "            modified_cosine_of_target_classes: (None,)\n",
    "        \"\"\"\n",
    "        eps = 1e-6\n",
    "        # theta in the paper\n",
    "        angles = torch.acos(torch.clamp(cosine_of_target_classes, -1 + eps, 1 - eps))\n",
    "        return torch.cos(angles + self.margin)\n",
    "    \n",
    "    def scale_logits(self, logits):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (None, n_classes)\n",
    "        Returns:\n",
    "            scaled_logits: (None, n_classes)\n",
    "        \"\"\"\n",
    "        return logits * self.scale\n",
    "    \n",
    "class SoftmaxLoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_size):\n",
    "        \"\"\"\n",
    "        Regular softmax loss (1 fc layer without bias + CrossEntropyLoss)\n",
    "        Args:\n",
    "            num_classes: The number of classes in your training dataset\n",
    "            embedding_size: The size of the embeddings that you pass into\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.Tensor(num_classes, embedding_size))\n",
    "        nn.init.xavier_normal_(self.W)\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (None, embedding_size)\n",
    "            labels: (None,)\n",
    "        Returns:\n",
    "            loss: scalar\n",
    "        \"\"\"\n",
    "        logits = F.linear(embeddings, self.W)\n",
    "        return nn.CrossEntropyLoss()(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0aa91cec-5d77-4431-9d41-dbb769f486e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class MobilenetEmbedding(nn.Module):\n",
    "#     def __init__(self, embedding_size=128, weights=None):\n",
    "#         super().__init__()\n",
    "#         self.mobilenet = mobilenet_v3_large(weights=weights)\n",
    "#         # self.mobilenet = mobilenet_v2(weights=weights) \n",
    "#         self.mobilenet.classifier[3] = nn.Linear(1280, embedding_size)\n",
    "#         if weights is not None:\n",
    "#             for param in self.mobilenet.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.mobilenet.classifier[3].parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.mobilenet(x)\n",
    "#         x = F.normalize(x, p=2, dim=1)\n",
    "#         return x\n",
    "\n",
    "class MobilenetEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_size=128, weights=None):\n",
    "        super().__init__()\n",
    "        # self.mobilenet = mobilenet_v3_large(weights=weights)\n",
    "        self.mobilenet = mobilenet_v2(weights=weights) \n",
    "        self.mobilenet.classifier[1] = nn.Linear(1280, embedding_size)\n",
    "        # if weights is not None:\n",
    "        #     for param in self.mobilenet.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        #     for param in self.mobilenet.classifier[1].parameters():\n",
    "        #         param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mobilenet(x)\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "235c065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: mobilenet.features.0.0.weight, Requires Grad: False, Shape: torch.Size([32, 3, 3, 3])\n",
      "Name: mobilenet.features.0.1.weight, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.0.1.bias, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.1.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([32, 1, 3, 3])\n",
      "Name: mobilenet.features.1.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.1.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.1.conv.1.weight, Requires Grad: False, Shape: torch.Size([16, 32, 1, 1])\n",
      "Name: mobilenet.features.1.conv.2.weight, Requires Grad: False, Shape: torch.Size([16])\n",
      "Name: mobilenet.features.1.conv.2.bias, Requires Grad: False, Shape: torch.Size([16])\n",
      "Name: mobilenet.features.2.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([96, 16, 1, 1])\n",
      "Name: mobilenet.features.2.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.2.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.2.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([96, 1, 3, 3])\n",
      "Name: mobilenet.features.2.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.2.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.2.conv.2.weight, Requires Grad: False, Shape: torch.Size([24, 96, 1, 1])\n",
      "Name: mobilenet.features.2.conv.3.weight, Requires Grad: False, Shape: torch.Size([24])\n",
      "Name: mobilenet.features.2.conv.3.bias, Requires Grad: False, Shape: torch.Size([24])\n",
      "Name: mobilenet.features.3.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: mobilenet.features.3.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.3.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.3.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: mobilenet.features.3.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.3.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.3.conv.2.weight, Requires Grad: False, Shape: torch.Size([24, 144, 1, 1])\n",
      "Name: mobilenet.features.3.conv.3.weight, Requires Grad: False, Shape: torch.Size([24])\n",
      "Name: mobilenet.features.3.conv.3.bias, Requires Grad: False, Shape: torch.Size([24])\n",
      "Name: mobilenet.features.4.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([144, 24, 1, 1])\n",
      "Name: mobilenet.features.4.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.4.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.4.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([144, 1, 3, 3])\n",
      "Name: mobilenet.features.4.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.4.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([144])\n",
      "Name: mobilenet.features.4.conv.2.weight, Requires Grad: False, Shape: torch.Size([32, 144, 1, 1])\n",
      "Name: mobilenet.features.4.conv.3.weight, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.4.conv.3.bias, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.5.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([192, 32, 1, 1])\n",
      "Name: mobilenet.features.5.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.5.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.5.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([192, 1, 3, 3])\n",
      "Name: mobilenet.features.5.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.5.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.5.conv.2.weight, Requires Grad: False, Shape: torch.Size([32, 192, 1, 1])\n",
      "Name: mobilenet.features.5.conv.3.weight, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.5.conv.3.bias, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.6.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([192, 32, 1, 1])\n",
      "Name: mobilenet.features.6.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.6.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.6.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([192, 1, 3, 3])\n",
      "Name: mobilenet.features.6.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.6.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.6.conv.2.weight, Requires Grad: False, Shape: torch.Size([32, 192, 1, 1])\n",
      "Name: mobilenet.features.6.conv.3.weight, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.6.conv.3.bias, Requires Grad: False, Shape: torch.Size([32])\n",
      "Name: mobilenet.features.7.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([192, 32, 1, 1])\n",
      "Name: mobilenet.features.7.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.7.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.7.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([192, 1, 3, 3])\n",
      "Name: mobilenet.features.7.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.7.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([192])\n",
      "Name: mobilenet.features.7.conv.2.weight, Requires Grad: False, Shape: torch.Size([64, 192, 1, 1])\n",
      "Name: mobilenet.features.7.conv.3.weight, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.7.conv.3.bias, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.8.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([384, 64, 1, 1])\n",
      "Name: mobilenet.features.8.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.8.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.8.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([384, 1, 3, 3])\n",
      "Name: mobilenet.features.8.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.8.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.8.conv.2.weight, Requires Grad: False, Shape: torch.Size([64, 384, 1, 1])\n",
      "Name: mobilenet.features.8.conv.3.weight, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.8.conv.3.bias, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.9.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([384, 64, 1, 1])\n",
      "Name: mobilenet.features.9.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.9.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.9.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([384, 1, 3, 3])\n",
      "Name: mobilenet.features.9.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.9.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.9.conv.2.weight, Requires Grad: False, Shape: torch.Size([64, 384, 1, 1])\n",
      "Name: mobilenet.features.9.conv.3.weight, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.9.conv.3.bias, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.10.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([384, 64, 1, 1])\n",
      "Name: mobilenet.features.10.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.10.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.10.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([384, 1, 3, 3])\n",
      "Name: mobilenet.features.10.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.10.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.10.conv.2.weight, Requires Grad: False, Shape: torch.Size([64, 384, 1, 1])\n",
      "Name: mobilenet.features.10.conv.3.weight, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.10.conv.3.bias, Requires Grad: False, Shape: torch.Size([64])\n",
      "Name: mobilenet.features.11.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([384, 64, 1, 1])\n",
      "Name: mobilenet.features.11.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.11.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.11.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([384, 1, 3, 3])\n",
      "Name: mobilenet.features.11.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.11.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([384])\n",
      "Name: mobilenet.features.11.conv.2.weight, Requires Grad: False, Shape: torch.Size([96, 384, 1, 1])\n",
      "Name: mobilenet.features.11.conv.3.weight, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.11.conv.3.bias, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.12.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([576, 96, 1, 1])\n",
      "Name: mobilenet.features.12.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.12.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.12.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([576, 1, 3, 3])\n",
      "Name: mobilenet.features.12.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.12.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.12.conv.2.weight, Requires Grad: False, Shape: torch.Size([96, 576, 1, 1])\n",
      "Name: mobilenet.features.12.conv.3.weight, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.12.conv.3.bias, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.13.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([576, 96, 1, 1])\n",
      "Name: mobilenet.features.13.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.13.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.13.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([576, 1, 3, 3])\n",
      "Name: mobilenet.features.13.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.13.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.13.conv.2.weight, Requires Grad: False, Shape: torch.Size([96, 576, 1, 1])\n",
      "Name: mobilenet.features.13.conv.3.weight, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.13.conv.3.bias, Requires Grad: False, Shape: torch.Size([96])\n",
      "Name: mobilenet.features.14.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([576, 96, 1, 1])\n",
      "Name: mobilenet.features.14.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.14.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.14.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([576, 1, 3, 3])\n",
      "Name: mobilenet.features.14.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.14.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([576])\n",
      "Name: mobilenet.features.14.conv.2.weight, Requires Grad: False, Shape: torch.Size([160, 576, 1, 1])\n",
      "Name: mobilenet.features.14.conv.3.weight, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.14.conv.3.bias, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.15.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([960, 160, 1, 1])\n",
      "Name: mobilenet.features.15.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.15.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.15.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([960, 1, 3, 3])\n",
      "Name: mobilenet.features.15.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.15.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.15.conv.2.weight, Requires Grad: False, Shape: torch.Size([160, 960, 1, 1])\n",
      "Name: mobilenet.features.15.conv.3.weight, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.15.conv.3.bias, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.16.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([960, 160, 1, 1])\n",
      "Name: mobilenet.features.16.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.16.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.16.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([960, 1, 3, 3])\n",
      "Name: mobilenet.features.16.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.16.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.16.conv.2.weight, Requires Grad: False, Shape: torch.Size([160, 960, 1, 1])\n",
      "Name: mobilenet.features.16.conv.3.weight, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.16.conv.3.bias, Requires Grad: False, Shape: torch.Size([160])\n",
      "Name: mobilenet.features.17.conv.0.0.weight, Requires Grad: False, Shape: torch.Size([960, 160, 1, 1])\n",
      "Name: mobilenet.features.17.conv.0.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.17.conv.0.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.17.conv.1.0.weight, Requires Grad: False, Shape: torch.Size([960, 1, 3, 3])\n",
      "Name: mobilenet.features.17.conv.1.1.weight, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.17.conv.1.1.bias, Requires Grad: False, Shape: torch.Size([960])\n",
      "Name: mobilenet.features.17.conv.2.weight, Requires Grad: False, Shape: torch.Size([320, 960, 1, 1])\n",
      "Name: mobilenet.features.17.conv.3.weight, Requires Grad: False, Shape: torch.Size([320])\n",
      "Name: mobilenet.features.17.conv.3.bias, Requires Grad: False, Shape: torch.Size([320])\n",
      "Name: mobilenet.features.18.0.weight, Requires Grad: False, Shape: torch.Size([1280, 320, 1, 1])\n",
      "Name: mobilenet.features.18.1.weight, Requires Grad: False, Shape: torch.Size([1280])\n",
      "Name: mobilenet.features.18.1.bias, Requires Grad: False, Shape: torch.Size([1280])\n",
      "Name: mobilenet.classifier.1.weight, Requires Grad: True, Shape: torch.Size([128, 1280])\n",
      "Name: mobilenet.classifier.1.bias, Requires Grad: True, Shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Name: {name}, Requires Grad: {param.requires_grad}, Shape: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96323f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:36<00:00,  1.11it/s]\n",
      "Epoch 1/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 11.7449 | Test Loss: 9.6676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 2/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.7450 | Test Loss: 4.4552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:22<00:00,  1.81it/s]\n",
      "Epoch 3/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 1.3139 | Test Loss: 2.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.77it/s]\n",
      "Epoch 4/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.4670 | Test Loss: 1.7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.65it/s]\n",
      "Epoch 5/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.2463 | Test Loss: 0.9994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.75it/s]\n",
      "Epoch 6/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.1338 | Test Loss: 1.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.72it/s]\n",
      "Epoch 7/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.1177 | Test Loss: 0.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 8/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.0753 | Test Loss: 0.7363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:22<00:00,  1.81it/s]\n",
      "Epoch 9/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.0433 | Test Loss: 0.8567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 10/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.0473 | Test Loss: 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.71it/s]\n",
      "Epoch 11/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.0675 | Test Loss: 0.8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.61it/s]\n",
      "Epoch 12/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.0407 | Test Loss: 0.7423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [00:17<00:22,  1.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m model.train()\n\u001b[32m     63\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m - Training\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1447\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1402\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1401\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1403\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1404\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1231\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1232\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1241\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1244\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1247\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1248\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda3\\envs\\deeplearning\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.3,\n",
    "    scale=30.0\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43be511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.73it/s]\n",
      "Epoch 1/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 11.1741 | Test Loss: 9.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.77it/s]\n",
      "Epoch 2/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 4.1952 | Test Loss: 4.6143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.76it/s]\n",
      "Epoch 3/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 1.1857 | Test Loss: 2.0590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 4/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.5585 | Test Loss: 1.4975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.76it/s]\n",
      "Epoch 5/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.2898 | Test Loss: 1.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 6/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.1331 | Test Loss: 0.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.78it/s]\n",
      "Epoch 7/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.0738 | Test Loss: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:22<00:00,  1.80it/s]\n",
      "Epoch 8/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.0624 | Test Loss: 0.8171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.74it/s]\n",
      "Epoch 9/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.0569 | Test Loss: 0.7826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 10/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.0516 | Test Loss: 0.6688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 11/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.0588 | Test Loss: 0.8207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 12/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.0553 | Test Loss: 0.6495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 13/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.0307 | Test Loss: 0.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.73it/s]\n",
      "Epoch 14/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.0381 | Test Loss: 0.7199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 15/15 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.0538 | Test Loss: 0.7997\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.3,\n",
    "    scale=30.0\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 15\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "96fcc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 17.7598 | Test Loss: 16.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 13.3814 | Test Loss: 13.4777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 7.8086 | Test Loss: 9.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.71it/s]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 3.9763 | Test Loss: 6.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 2.0972 | Test Loss: 4.3144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.68it/s]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.1835 | Test Loss: 3.5472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.55it/s]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.8025 | Test Loss: 2.9115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.58it/s]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.5074 | Test Loss: 2.6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.58it/s]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.4743 | Test Loss: 1.8817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.2710 | Test Loss: 1.9759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:23<00:00,  1.71it/s]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2281 | Test Loss: 1.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.1962 | Test Loss: 2.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.64it/s]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.1630 | Test Loss: 1.5759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.1556 | Test Loss: 1.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.1303 | Test Loss: 1.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 16/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.0919 | Test Loss: 1.3975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.69it/s]\n",
      "Epoch 17/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.1293 | Test Loss: 1.3615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.70it/s]\n",
      "Epoch 18/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:18<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.1414 | Test Loss: 1.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.65it/s]\n",
      "Epoch 19/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.1005 | Test Loss: 1.4541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.54it/s]\n",
      "Epoch 20/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 | Train Loss: 0.0809 | Test Loss: 1.1434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.59it/s]\n",
      "Epoch 21/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 | Train Loss: 0.0490 | Test Loss: 1.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 22/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 | Train Loss: 0.0544 | Test Loss: 1.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:29<00:00,  1.39it/s]\n",
      "Epoch 23/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 | Train Loss: 0.0875 | Test Loss: 1.2250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 24/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 | Train Loss: 0.1289 | Test Loss: 1.3865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.65it/s]\n",
      "Epoch 25/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 | Train Loss: 0.0814 | Test Loss: 1.2364\n",
      "Early stopping triggered after 25 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V1\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1.5e-4, weight_decay=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1672f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 18.0244 | Test Loss: 16.5904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:28<00:00,  1.43it/s]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 12.8248 | Test Loss: 12.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:30<00:00,  1.35it/s]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 5.9796 | Test Loss: 7.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 2.6455 | Test Loss: 4.1568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.66it/s]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 1.3765 | Test Loss: 2.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.64it/s]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 1.0202 | Test Loss: 2.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.63it/s]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.6132 | Test Loss: 1.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:24<00:00,  1.67it/s]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:19<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.5023 | Test Loss: 2.0471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:25<00:00,  1.62it/s]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.4219 | Test Loss: 1.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.57it/s]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.3444 | Test Loss: 1.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2669 | Test Loss: 1.4840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.53it/s]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.4428 | Test Loss: 1.9592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.57it/s]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.4278 | Test Loss: 1.4510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.55it/s]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.3367 | Test Loss: 1.3939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.51it/s]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.3294 | Test Loss: 1.3997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:28<00:00,  1.42it/s]\n",
      "Epoch 16/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Train Loss: 0.3273 | Test Loss: 1.5632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:26<00:00,  1.53it/s]\n",
      "Epoch 17/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Train Loss: 0.2587 | Test Loss: 1.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.49it/s]\n",
      "Epoch 18/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:20<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Train Loss: 0.1835 | Test Loss: 1.6515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [00:27<00:00,  1.50it/s]\n",
      "Epoch 19/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:21<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Train Loss: 0.2681 | Test Loss: 1.8248\n",
      "Early stopping triggered after 19 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## INI NYOBA MENAMBAH transformation\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobilenetEmbedding(\n",
    "    embedding_size=128,\n",
    "    weights=MobileNet_V2_Weights.IMAGENET1K_V2\n",
    ").to(device)\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=128,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10**LR_MIN_EXP, 10**LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10**WD_MIN_EXP, 10**WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d70b0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_13204\\565060365.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prototype-based cosine similarity accuracy: 97.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Model ---\n",
    "# model = MobilenetEmbedding(embedding_size=128).to(device)\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_2.pth\", weights_only=True))\n",
    "\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "model.eval()\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Extract train embeddings ---\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_train.append(emb.cpu().numpy())\n",
    "        y_train.append(labels.numpy())\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "# --- Extract test embeddings ---\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_test.append(emb.cpu().numpy())\n",
    "        y_test.append(labels.numpy())\n",
    "\n",
    "X_test = np.vstack(X_test)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "# --- Build class prototypes ---\n",
    "classes = np.unique(y_train)\n",
    "# Build prototypes with class indices aligned to ImageFolder\n",
    "prototypes = {}\n",
    "for idx, cls in enumerate(train_dataset.classes):  # use ImageFolder.classes\n",
    "    class_idx = train_dataset.class_to_idx[cls]\n",
    "    prototypes[class_idx] = X_train[y_train == class_idx].mean(axis=0)\n",
    "    prototypes[class_idx] /= np.linalg.norm(prototypes[class_idx])\n",
    "\n",
    "# --- Prediction function ---\n",
    "def predict_embedding(embedding, prototypes, threshold=0.5):\n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    best_cls = None\n",
    "    best_sim = -1\n",
    "    for cls_idx, proto in prototypes.items():\n",
    "        sim = np.dot(embedding, proto)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_cls = cls_idx\n",
    "    if best_sim < threshold:\n",
    "        return -1, best_sim  # Unknown\n",
    "    return best_cls, best_sim\n",
    "\n",
    "# --- Predict on test set ---\n",
    "predictions = []\n",
    "similarities = []\n",
    "\n",
    "for emb in X_test:\n",
    "    cls, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "    predictions.append(cls)\n",
    "    similarities.append(sim)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "similarities = np.array(similarities)\n",
    "\n",
    "# --- Accuracy ---\n",
    "acc = np.mean(predictions == y_test)\n",
    "print(f\"âœ… Prototype-based cosine similarity accuracy: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed3b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_13204\\3347148874.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset pelatihan untuk membangun prototipe...\n",
      "Ditemukan 54 kelas: ['5221911012_Debora', '5221911025_Anggun', '5231811002_MichaelAndrewDeHaan', '5231811004_Hamdanu Fahmi Utomo', '5231811005_Akhmad Nabil Saputra', '5231811006_Daniel Granesa Kiara', '5231811007_Amalia Dwi Ramadhani', '5231811008_Sophia', '5231811009_Otniel Chresto Purwandi', '5231811010_MEYLAN ARYANI', '5231811013_KusumaRatih', '5231811014_Dian Eka Pratiwi', '5231811015_Fadilah Ratu Azzahra', '5231811016_Kesya', '5231811017_Maulana Ahmad Muhaimin', '5231811018_Sulis Septiani Putri', '5231811019_Chronika', '5231811021_NASHA SHINTA ABP', '5231811022_Lathif Ramadhan', '5231811023_Rahma Fieka Januarni', '5231811024_Maria Febronia Boa', '5231811025_Novera', '5231811026_ULFAH NAFIAH', '5231811027_Naufal', \"5231811028_Ma'ruf Ndaru Sasono\", '5231811029_Andini', '5231811030_Antonia Idan Huler', '5231811031_CindyKusumaningrum', '5231811033_Rama Panji Nararendra Cahaya', '5231811034_Shilsylia Putri Devitasary', '5231811035_Yogi Hanusanjaya', '5231811036_Giffari Riyanda Pradithya', '5231811037_Muhammad Ibra Ramadhon', '5231811038_yudit manda', '5231811039_Rambang Widyadana', '5231911001_Farma', '5231911002_Desak Gde Devika Anantha Armi', '5231911003_ulen', '5231911004_al faisal selan', '5231911005_Deraya', '5231911006_ARI DHARMA DEVANANTA', '5231911007_julia eliza w', '5231911008_Faren', '5231911009_Ica Nurcahya', '5231911010_Jeny Amelindrika Putri', '5231911011_Nirmala Fitri Margitaya', '5231911012_Thalita Revalyna Maharani', '5231911013_REVA SEBRINA SALSABILA', '5231911014_Asti Eka Rahayu', '5231911016_Ameliawati', '5231911017_RAFI AUFA MIRZA', '5231911018_FIRMAN', '5231911019_Glory Valentio Duska Putra', '5231911020_siti nabila maulidya']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# --- Variabel & Path (dari skrip Anda) ---\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_1.pth\"\n",
    "# model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best_1.pth\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# --- Model ---\n",
    "# model = MobilenetEmbedding(embedding_size=128).to(device)\n",
    "# # model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_best.pth\", weights_only=True))\n",
    "# model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_moreagument_best.pth\", weights_only=True))\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- Transformasi ---\n",
    "# Transformasi untuk model embedding\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 1: MEMBANGUN PROTOTIPE KELAS (Dijalankan sekali saat startup)\n",
    "# ==============================================================================\n",
    "print(\"Loading dataset pelatihan untuk membangun prototipe...\")\n",
    "\n",
    "# --- Datasets & Dataloaders (Hanya untuk train set) ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Dapatkan nama kelas\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Ditemukan {len(class_names)} kelas: {class_names}\")\n",
    "\n",
    "# --- Ekstraksi embedding pelatihan ---\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        emb = model(imgs)\n",
    "        X_train.append(emb.cpu().numpy())\n",
    "        y_train.append(labels.numpy())\n",
    "\n",
    "X_train = np.vstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "\n",
    "print(f\"Ekstraksi {len(X_train)} embedding pelatihan selesai.\")\n",
    "\n",
    "# --- Membangun prototipe kelas (dari skrip Anda) ---\n",
    "prototypes = {}\n",
    "for idx, cls in enumerate(train_dataset.classes):  # use ImageFolder.classes\n",
    "    class_idx = train_dataset.class_to_idx[cls]\n",
    "    class_embeddings = X_train[y_train == class_idx]\n",
    "    \n",
    "    if len(class_embeddings) > 0:\n",
    "        prototypes[class_idx] = class_embeddings.mean(axis=0)\n",
    "        prototypes[class_idx] /= np.linalg.norm(prototypes[class_idx])\n",
    "    else:\n",
    "        print(f\"Peringatan: Tidak ada sampel ditemukan untuk kelas {cls} (idx: {class_idx})\")\n",
    "\n",
    "print(f\"Prototipe untuk {len(prototypes)} kelas berhasil dibuat.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 2: FUNGSI HELPER\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Fungsi Prediksi (dari skrip Anda) ---\n",
    "def predict_embedding(embedding, prototypes, threshold=0.8):\n",
    "    if embedding.ndim == 0 or np.linalg.norm(embedding) == 0:\n",
    "        return -1, 0.0 # Embedding tidak valid\n",
    "    \n",
    "    embedding = embedding / np.linalg.norm(embedding)\n",
    "    best_cls = None\n",
    "    best_sim = -1\n",
    "    \n",
    "    for cls_idx, proto in prototypes.items():\n",
    "        sim = np.dot(embedding, proto)\n",
    "        if sim > best_sim:\n",
    "            best_sim = sim\n",
    "            best_cls = cls_idx\n",
    "            \n",
    "    if best_sim < threshold:\n",
    "        return -1, best_sim  # Unknown\n",
    "        \n",
    "    return best_cls, best_sim\n",
    "\n",
    "# --- Fungsi Pencerah Gambar ---\n",
    "def auto_adjust_brightness(frame, target_brightness=90, low_threshold=70):\n",
    "    \"\"\"Mencerahkan frame jika brightness rata-rata di bawah ambang batas.\"\"\"\n",
    "    \n",
    "    # Konversi ke Grayscale untuk mengecek brightness\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    \n",
    "    if mean_brightness < low_threshold:\n",
    "        # Hitung seberapa banyak brightness perlu ditambah\n",
    "        delta = target_brightness - mean_brightness\n",
    "        \n",
    "        # Tambah brightness menggunakan cv2.convertScaleAbs untuk penanganan saturasi\n",
    "        # alpha=1 (kontras tetap), beta=delta (brightness ditambah)\n",
    "        bright_frame = cv2.convertScaleAbs(frame, alpha=1.0, beta=delta)\n",
    "        return bright_frame\n",
    "        \n",
    "    return frame\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 3: REAL-TIME LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Inisialisasi MTCNN ---\n",
    "mtcnn = MTCNN(\n",
    "    keep_all=True,              # Deteksi semua wajah\n",
    "    post_process=False,         # Jangan lakukan normalisasi (kita lakukan di transform)\n",
    "    min_face_size=40,           # Abaikan wajah yang sangat kecil\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Inisialisasi Video Capture ---\n",
    "# Ganti URL ini dengan URL dari aplikasi IP Webcam Anda\n",
    "# Contoh: \"http://192.168.1.10:8080/video\"\n",
    "# video_url = \"http://YOUR_PHONE_IP:8080/video\"\n",
    "\n",
    "# Untuk testing menggunakan Webcam internal, ganti 'video_url' dengan 0\n",
    "# cap = cv2.VideoCapture(0) \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Tidak bisa membuka stream video.\")\n",
    "    print(f\"Pastikan URL '{video_url}' benar dan aplikasi IP Webcam berjalan.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Memulai stream video... Tekan 'q' untuk keluar.\")\n",
    "\n",
    "# Variabel untuk FPS\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Gagal membaca frame, mencoba menghubungkan kembali...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(video_url)\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    # 1. Pencerahan Otomatis (Sesuai permintaan Anda)\n",
    "    frame = auto_adjust_brightness(frame)\n",
    "\n",
    "    # 2. Konversi BGR (OpenCV) ke RGB (PIL) untuk MTCNN\n",
    "    # MTCNN mengharapkan input PIL Image\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 3. Deteksi Wajah dengan MTCNN\n",
    "    boxes, _, landmarks = mtcnn.detect(img_pil, landmarks=True)\n",
    "\n",
    "    # 4. Loop untuk setiap wajah yang terdeteksi\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):  # 'i' ditambahkan di sini\n",
    "            x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "            # Pastikan koordinat valid (tidak keluar dari frame)\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(frame.shape[1] - 1, x2)\n",
    "            y2 = min(frame.shape[0] - 1, y2)\n",
    "\n",
    "            # Pastikan crop valid\n",
    "            if y2 <= y1 or x2 <= x1:\n",
    "                continue\n",
    "\n",
    "            # 5. Crop wajah dari frame BGR asli\n",
    "            face_crop_bgr = frame[y1:y2, x1:x2]\n",
    "\n",
    "\n",
    "            # 5.5 ALIGN Wajah menggunakan Landmark Mata\n",
    "            try:\n",
    "                # Ambil landmark untuk wajah ini\n",
    "                current_landmarks = landmarks[i]\n",
    "                left_eye = current_landmarks[0]   # [x, y] mata kiri\n",
    "                right_eye = current_landmarks[1]  # [x, y] mata kanan\n",
    "                \n",
    "                # Hitung sudut antara dua mata\n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                # Hitung sudut dalam derajat\n",
    "                angle_deg = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "                # Dapatkan center dari face crop (untuk titik rotasi)\n",
    "                (h_crop, w_crop) = face_crop_bgr.shape[:2]\n",
    "                center_crop = (w_crop // 2, h_crop // 2)\n",
    "\n",
    "                # Dapatkan matriks rotasi\n",
    "                M = cv2.getRotationMatrix2D(center_crop, angle_deg, 1.0)\n",
    "                \n",
    "                # Terapkan rotasi\n",
    "                # cv2.INTER_CUBIC memberikan kualitas lebih baik\n",
    "                # borderMode=cv2.BORDER_CONSTANT mengisi area hitam jika ada\n",
    "                aligned_face_crop = cv2.warpAffine(face_crop_bgr, M, (w_crop, h_crop), \n",
    "                                                flags=cv2.INTER_CUBIC, \n",
    "                                                borderMode=cv2.BORDER_CONSTANT,\n",
    "                                                borderValue=(0, 0, 0))\n",
    "                \n",
    "                # Gunakan wajah yang sudah di-align\n",
    "                face_to_process = aligned_face_crop\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Jika gagal (misal landmark tidak terdeteksi), pakai crop asli\n",
    "                face_to_process = face_crop_bgr\n",
    "\n",
    "            # 6. Preprocessing wajah untuk model\n",
    "            # Konversi crop (BGR) ke PIL (RGB)\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_crop_bgr, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Terapkan transformasi\n",
    "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # 7. Dapatkan Embedding\n",
    "            with torch.no_grad():\n",
    "                emb = model(face_tensor)\n",
    "            \n",
    "            emb = emb.cpu().numpy().squeeze()\n",
    "\n",
    "            # 8. Prediksi menggunakan Prototipe\n",
    "            cls_idx, sim = predict_embedding(emb, prototypes, threshold=0.5)\n",
    "\n",
    "            # 9. Dapatkan Label Nama\n",
    "            if cls_idx == -1:\n",
    "                label = \"Unknown\"\n",
    "                color = (0, 0, 255) # Merah\n",
    "            else:\n",
    "                label = class_names[cls_idx]\n",
    "                color = (0, 255, 0) # Hijau\n",
    "            \n",
    "            display_text = f\"{label} ({sim*100:.1f}%)\"\n",
    "\n",
    "            # 10. Gambar Bounding Box dan Label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # Hitung dan tampilkan FPS\n",
    "    frame_count += 1\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    if elapsed >= 1.0:\n",
    "        fps = frame_count / elapsed\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow('Real-time Face Recognition (Tekan q untuk keluar)', frame)\n",
    "\n",
    "    # Tombol Keluar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Stream dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e8c2a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n",
      "Loading class names from dataset directory...\n",
      "Ditemukan 54 kelas: ['5221911012_Debora', '5221911025_Anggun', '5231811002_MichaelAndrewDeHaan', '5231811004_Hamdanu Fahmi Utomo', '5231811005_Akhmad Nabil Saputra', '5231811006_Daniel Granesa Kiara', '5231811007_Amalia Dwi Ramadhani', '5231811008_Sophia', '5231811009_Otniel Chresto Purwandi', '5231811010_MEYLAN ARYANI', '5231811013_KusumaRatih', '5231811014_Dian Eka Pratiwi', '5231811015_Fadilah Ratu Azzahra', '5231811016_Kesya', '5231811017_Maulana Ahmad Muhaimin', '5231811018_Sulis Septiani Putri', '5231811019_Chronika', '5231811021_NASHA SHINTA ABP', '5231811022_Lathif Ramadhan', '5231811023_Rahma Fieka Januarni', '5231811024_Maria Febronia Boa', '5231811025_Novera', '5231811026_ULFAH NAFIAH', '5231811027_Naufal', \"5231811028_Ma'ruf Ndaru Sasono\", '5231811029_Andini', '5231811030_Antonia Idan Huler', '5231811031_CindyKusumaningrum', '5231811033_Rama Panji Nararendra Cahaya', '5231811034_Shilsylia Putri Devitasary', '5231811035_Yogi Hanusanjaya', '5231811036_Giffari Riyanda Pradithya', '5231811037_Muhammad Ibra Ramadhon', '5231811038_yudit manda', '5231811039_Rambang Widyadana', '5231911001_Farma', '5231911002_Desak Gde Devika Anantha Armi', '5231911003_ulen', '5231911004_al faisal selan', '5231911005_Deraya', '5231911006_ARI DHARMA DEVANANTA', '5231911007_julia eliza w', '5231911008_Faren', '5231911009_Ica Nurcahya', '5231911010_Jeny Amelindrika Putri', '5231911011_Nirmala Fitri Margitaya', '5231911012_Thalita Revalyna Maharani', '5231911013_REVA SEBRINA SALSABILA', '5231911014_Asti Eka Rahayu', '5231911016_Ameliawati', '5231911017_RAFI AUFA MIRZA', '5231911018_FIRMAN', '5231911019_Glory Valentio Duska Putra', '5231911020_siti nabila maulidya']\n",
      "Loading model weights from: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\best_mobilenetv2_face.pth\n",
      "Model classifier berhasil dimuat.\n",
      "Memulai stream video... Tekan 'q' untuk keluar.\n",
      "Stream dihentikan.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import mobilenet_v2 # <-- IMPORT BARU\n",
    "from torch.utils.data import DataLoader\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# --- Variabel & Path ---\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "# Ganti dengan path model classifier Anda\n",
    "model_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\best_mobilenetv2_face.pth\" \n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 1: MEMUAT MODEL CLASSIFIER DAN NAMA KELAS\n",
    "# ==============================================================================\n",
    "print(\"Loading class names from dataset directory...\")\n",
    "# Kita perlu dataset HANYA untuk mendapatkan nama kelas\n",
    "temp_dataset = datasets.ImageFolder(root=train_dir)\n",
    "class_names = temp_dataset.classes\n",
    "num_classes = len(class_names)\n",
    "del temp_dataset # Hapus, tidak dipakai lagi\n",
    "\n",
    "print(f\"Ditemukan {num_classes} kelas: {class_names}\")\n",
    "\n",
    "# --- Load model pre-trained (Sesuai permintaan Anda) ---\n",
    "model = mobilenet_v2(weights='IMAGENET1K_V1')\n",
    "num_features = model.classifier[1].in_features\n",
    "\n",
    "# --- Ganti classifier terakhir ---\n",
    "model.classifier[1] = nn.Linear(num_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Load state dictionary Anda ---\n",
    "print(f\"Loading model weights from: {model_path}\")\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()\n",
    "print(\"Model classifier berhasil dimuat.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 2: TRANSFORMASI\n",
    "# ==============================================================================\n",
    "# Transformasi untuk model (sama seperti sebelumnya)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 3: FUNGSI HELPER (Hanya Pencerah)\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Fungsi Pencerah Gambar ---\n",
    "def auto_adjust_brightness(frame, target_brightness=90, low_threshold=70):\n",
    "    \"\"\"Mencerahkan frame jika brightness rata-rata di bawah ambang batas.\"\"\"\n",
    "    \n",
    "    # Konversi ke Grayscale untuk mengecek brightness\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    mean_brightness = np.mean(gray)\n",
    "    \n",
    "    if mean_brightness < low_threshold:\n",
    "        # Hitung seberapa banyak brightness perlu ditambah\n",
    "        delta = target_brightness - mean_brightness\n",
    "        \n",
    "        # Tambah brightness menggunakan cv2.convertScaleAbs\n",
    "        bright_frame = cv2.convertScaleAbs(frame, alpha=1.0, beta=delta)\n",
    "        return bright_frame\n",
    "        \n",
    "    return frame\n",
    "\n",
    "# ==============================================================================\n",
    "# TAHAP 4: REAL-TIME LOOP\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Inisialisasi MTCNN ---\n",
    "mtcnn = MTCNN(\n",
    "    keep_all=True,              # Deteksi semua wajah\n",
    "    post_process=False,         # Jangan lakukan normalisasi\n",
    "    min_face_size=40,           # Abaikan wajah yang sangat kecil\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- Inisialisasi Video Capture ---\n",
    "cap = cv2.VideoCapture(0) # Gunakan webcam internal\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Tidak bisa membuka webcam (index 0).\")\n",
    "    exit()\n",
    "\n",
    "print(\"Memulai stream video... Tekan 'q' untuk keluar.\")\n",
    "\n",
    "# Variabel untuk FPS\n",
    "frame_count = 0\n",
    "start_time = time.time()\n",
    "# Tentukan threshold kepercayaan (confidence)\n",
    "confidence_threshold = 0.2 # 75% - Sesuaikan jika perlu\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Gagal membaca frame.\")\n",
    "        break\n",
    "\n",
    "    # 1. Pencerahan Otomatis (Sesuai permintaan Anda)\n",
    "    frame_bright = auto_adjust_brightness(frame)\n",
    "\n",
    "    # 2. Konversi BGR (OpenCV) ke RGB (PIL) untuk MTCNN\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(frame_bright, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # 3. Deteksi Wajah dengan MTCNN (dan dapatkan landmarks)\n",
    "    boxes, _, landmarks = mtcnn.detect(img_pil, landmarks=True)\n",
    "\n",
    "    # 4. Loop untuk setiap wajah yang terdeteksi\n",
    "    if boxes is not None:\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = [int(b) for b in box]\n",
    "\n",
    "            # Sanitasi koordinat\n",
    "            x1 = max(0, x1)\n",
    "            y1 = max(0, y1)\n",
    "            x2 = min(frame.shape[1] - 1, x2)\n",
    "            y2 = min(frame.shape[0] - 1, y2)\n",
    "\n",
    "            if y2 <= y1 or x2 <= x1:\n",
    "                continue\n",
    "\n",
    "            # 5. Crop wajah dari frame BGR asli\n",
    "            face_crop_bgr = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # 5.5 ALIGN Wajah menggunakan Landmark Mata\n",
    "            try:\n",
    "                current_landmarks = landmarks[i]\n",
    "                left_eye = current_landmarks[0]\n",
    "                right_eye = current_landmarks[1]\n",
    "                \n",
    "                delta_y = right_eye[1] - left_eye[1]\n",
    "                delta_x = right_eye[0] - left_eye[0]\n",
    "                angle_deg = np.degrees(np.arctan2(delta_y, delta_x))\n",
    "\n",
    "                (h_crop, w_crop) = face_crop_bgr.shape[:2]\n",
    "                center_crop = (w_crop // 2, h_crop // 2)\n",
    "\n",
    "                M = cv2.getRotationMatrix2D(center_crop, angle_deg, 1.0)\n",
    "                \n",
    "                aligned_face_crop = cv2.warpAffine(face_crop_bgr, M, (w_crop, h_crop), \n",
    "                                                   flags=cv2.INTER_CUBIC, \n",
    "                                                   borderMode=cv2.BORDER_CONSTANT)\n",
    "                \n",
    "                face_to_process = aligned_face_crop\n",
    "            \n",
    "            except Exception as e:\n",
    "                # Jika gagal align, pakai crop asli\n",
    "                face_to_process = face_crop_bgr\n",
    "            \n",
    "            # 6. Preprocessing wajah untuk model\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_to_process, cv2.COLOR_BGR2RGB))\n",
    "            face_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "            # --- â–¼â–¼â–¼ LOGIKA PREDIKSI BARU â–¼â–¼â–¼ ---\n",
    "            # 7. Dapatkan Prediksi (Logits)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(face_tensor) # Output adalah [1, num_classes]\n",
    "                \n",
    "                # 8. Ubah Logits ke Probabilitas (Softmax)\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # 9. Dapatkan kelas dan kepercayaan (confidence) tertinggi\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                cls_idx = predicted_idx.item()\n",
    "                conf = confidence.item()\n",
    "\n",
    "            # 10. Dapatkan Label Nama berdasarkan threshold\n",
    "            if conf < confidence_threshold:\n",
    "                label = \"Unknown\"\n",
    "                color = (0, 0, 255) # Merah\n",
    "            else:\n",
    "                label = class_names[cls_idx]\n",
    "                color = (0, 255, 0) # Hijau\n",
    "            \n",
    "            display_text = f\"{label} ({conf*100:.1f}%)\"\n",
    "            # --- â–²â–²â–² AKHIR LOGIKA PREDIKSI BARU â–²â–²â–² ---\n",
    "\n",
    "            # 11. Gambar Bounding Box dan Label\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, display_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "    # Hitung dan tampilkan FPS\n",
    "    frame_count += 1\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    if elapsed >= 1.0:\n",
    "        fps = frame_count / elapsed\n",
    "        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        frame_count = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow('Real-time Face Recognition (Classifier) - Tekan q', frame)\n",
    "\n",
    "    # Tombol Keluar\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Stream dihentikan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d0ecad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean intra-class sim: 0.9285825\n",
      "Mean inter-class sim: 0.0108167315\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Example: check intra-class vs inter-class similarities\n",
    "prototypes_norm = {k: v/np.linalg.norm(v) for k,v in prototypes.items()}\n",
    "\n",
    "intra_sims = []\n",
    "inter_sims = []\n",
    "\n",
    "for cls_idx, proto in prototypes_norm.items():\n",
    "    emb_cls = X_train[y_train == cls_idx]\n",
    "    emb_cls = emb_cls / np.linalg.norm(emb_cls, axis=1, keepdims=True)\n",
    "    # intra-class\n",
    "    for i,j in combinations(range(len(emb_cls)), 2):\n",
    "        intra_sims.append(np.dot(emb_cls[i], emb_cls[j]))\n",
    "    # inter-class\n",
    "    for other_idx, other_proto in prototypes_norm.items():\n",
    "        if other_idx != cls_idx:\n",
    "            inter_sims.append(np.dot(proto, other_proto))\n",
    "\n",
    "print(\"Mean intra-class sim:\", np.mean(intra_sims))\n",
    "print(\"Mean inter-class sim:\", np.mean(inter_sims))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e583c55a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m image = preprocess_image(image, brightness_thresh=\u001b[32m0.5\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Apply same preprocessing as your train/test set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m img_tensor = \u001b[43mtest_transform\u001b[49m(image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Extract embedding\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'test_transform' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# --- Folder path ---\n",
    "# img_folder = r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\\WhatsApp Image 2025_\"\n",
    "img_folder = r\"D:\\download_d\\data_test\"\n",
    "\n",
    "# --- Helper function: conditional preprocessing ---\n",
    "def preprocess_image(image_pil, brightness_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Convert to RGB PIL image -> possibly brighten if too dark -> return PIL image.\n",
    "    \"\"\"\n",
    "    # Convert PIL to OpenCV (numpy array)\n",
    "    img = np.array(image_pil)  # shape H,W,3, RGB\n",
    "    img_cv = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Convert to float [0,1]\n",
    "    img_float = img_cv.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Compute brightness (average luminance)\n",
    "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "    brightness = np.mean(gray) / 255.0\n",
    "    \n",
    "    # If too dark, adjust brightness & contrast\n",
    "    if brightness < brightness_thresh:\n",
    "        alpha = 1.2  # contrast\n",
    "        beta = 0.1   # brightness\n",
    "        img_float = np.clip(alpha * img_float + beta, 0, 1)\n",
    "        # Optional gamma correction\n",
    "        gamma = 1.2\n",
    "        img_float = np.power(img_float, 1/gamma)\n",
    "    \n",
    "    # Convert back to uint8 and PIL\n",
    "    img_float = (img_float * 255).astype(np.uint8)\n",
    "    img_rgb = cv2.cvtColor(img_float, cv2.COLOR_BGR2RGB)\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    \n",
    "    return img_pil\n",
    "\n",
    "# Get all image files\n",
    "img_paths = [os.path.join(img_folder, f) \n",
    "             for f in os.listdir(img_folder) \n",
    "             if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Process all images\n",
    "for img_path in img_paths:\n",
    "    # Load image\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    # Conditional preprocessing\n",
    "    image = preprocess_image(image, brightness_thresh=0.5)\n",
    "    \n",
    "    # Apply same preprocessing as your train/test set\n",
    "    img_tensor = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Extract embedding\n",
    "    with torch.no_grad():\n",
    "        embedding = model(img_tensor).cpu().numpy()[0]  # shape (128,)\n",
    "\n",
    "    # Predict using prototypes\n",
    "    predicted_cls, sim = predict_embedding(embedding, prototypes, threshold=0.6)  # adjust threshold\n",
    "    if predicted_cls == -1:\n",
    "        predicted_name = \"Unknown\"\n",
    "    else:\n",
    "        predicted_name = train_dataset.classes[predicted_cls]\n",
    "\n",
    "    print(f\"Image: {img_path} | Predicted: {predicted_name} | Cosine sim: {sim:.3f}\")\n",
    "\n",
    "    # Display image with label\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted: {predicted_name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "013b8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "d:\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "cropper = FaceCropper()\n",
    "cropper.process_folder(r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\", r\"C:\\Users\\ramap\\OneDrive\\Documents\\Kuliah\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "591dee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "from torch import nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "##################################  MobileFaceNet #############################################################\n",
    "    \n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "     def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "     def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, 512, bias=False)\n",
    "        self.bn = BatchNorm1d(512)\n",
    "        \n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(\"Input:\", x.shape)\n",
    "    \n",
    "        out = self.conv1(x)\n",
    "        # print(\"After conv1:\", out.shape)\n",
    "        \n",
    "        out = self.conv2_dw(out)\n",
    "        # print(\"After conv2_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "        # print(\"After conv_23:\", out.shape)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "        # print(\"After conv_3:\", out.shape)\n",
    "        \n",
    "        out = self.conv_34(out)\n",
    "        # print(\"After conv_34:\", out.shape)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "        # print(\"After conv_4:\", out.shape)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "        # print(\"After conv_45:\", out.shape)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "        # print(\"After conv_5:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "        # print(\"After conv_6_sep:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "        # print(\"After conv_6_dw:\", out.shape)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "        # print(\"After conv_6_flatten:\", out.shape)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        # print(\"After linear:\", out.shape)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        # print(\"After batchnorm:\", out.shape)\n",
    "\n",
    "        out = l2_norm(out)\n",
    "        # print(\"After l2_norm:\", out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "94514e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input = torch.Tensor(2, 3, 112, 112).to(device)\n",
    "net = MobileFaceNet(512).to(device)\n",
    "x = net(input)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea3ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramap\\AppData\\Local\\Temp\\ipykernel_13204\\3093528768.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
      "Epoch 1/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [03:12<00:00,  4.70s/it]\n",
      "Epoch 1/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:38<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 16.0721 | Test Loss: 13.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.84s/it]\n",
      "Epoch 2/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 9.0412 | Test Loss: 6.7330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.85s/it]\n",
      "Epoch 3/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 3.5873 | Test Loss: 3.1344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:19<00:00,  1.93s/it]\n",
      "Epoch 4/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:27<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 1.4521 | Test Loss: 1.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:46<00:00,  2.59s/it]\n",
      "Epoch 5/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.8508 | Test Loss: 1.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:17<00:00,  1.89s/it]\n",
      "Epoch 6/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.6205 | Test Loss: 1.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:14<00:00,  1.82s/it]\n",
      "Epoch 7/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:29<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.3514 | Test Loss: 1.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:25<00:00,  2.08s/it]\n",
      "Epoch 8/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.2911 | Test Loss: 1.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:17<00:00,  1.89s/it]\n",
      "Epoch 9/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:24<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.2418 | Test Loss: 1.1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:15<00:00,  1.85s/it]\n",
      "Epoch 10/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.2387 | Test Loss: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:16<00:00,  1.87s/it]\n",
      "Epoch 11/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Train Loss: 0.2610 | Test Loss: 1.0960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:16<00:00,  1.86s/it]\n",
      "Epoch 12/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Train Loss: 0.2405 | Test Loss: 1.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [02:09<00:00,  3.15s/it]\n",
      "Epoch 13/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:28<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Train Loss: 0.2368 | Test Loss: 1.4209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:18<00:00,  1.92s/it]\n",
      "Epoch 14/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:33<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Train Loss: 0.1960 | Test Loss: 1.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 - Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [01:45<00:00,  2.57s/it]\n",
      "Epoch 15/50 - Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:25<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Train Loss: 0.2686 | Test Loss: 1.4817\n",
      "Early stopping triggered after 15 epochs.\n",
      "âœ… Best model saved to: D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\n"
     ]
    }
   ],
   "source": [
    "## INI NYOBA MENAMBAH PAKE LAIN\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "# train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped OLD classed\"\n",
    "# test_dir  = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped OLD classed\"\n",
    "train_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Train Cropped MTCNN fix\"\n",
    "test_dir = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\data\\Dataset Sistem Presensi Wajah V2.0\\Data Test Cropped MTCNN fix\"\n",
    "\n",
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomGrayscale(p=0.2), # 20% kemungkinan gambar akan diubah jadi grayscale\n",
    "    transforms.ColorJitter(brightness=(0.2, 1.3),  # Kecerahan antara 40% - 100% (tidak pernah lebih terang)\n",
    "                            contrast=(0.7, 1.3),  # Sedikit variasi kontras\n",
    "                            saturation=0.2, \n",
    "                            hue=0.1),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.05,0.05), scale=(0.9,1.1), shear=5),\n",
    "    transforms.GaussianBlur(kernel_size=(3,3), sigma=(0.1,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Datasets ---\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
    "\n",
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = MobileFaceNet(512).to(device)\n",
    "model.load_state_dict(torch.load(r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\MobileFace_Net\", map_location=lambda storage, loc: storage))\n",
    "\n",
    "# --- Loss & Optimizer ---\n",
    "criterion = ArcFaceLoss(\n",
    "    num_classes=54,\n",
    "    embedding_size=512,\n",
    "    margin=0.5,   # standard margin; increases angular separation\n",
    "    scale=30.0    # keep the same\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005598380474533065, weight_decay=0.0005501701817196964)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), \n",
    "#                             lr=0.1, \n",
    "#                             momentum=0.9, \n",
    "#                             weight_decay=5e-4)\n",
    "# --- Training Loop ---\n",
    "num_epochs = 50\n",
    "early_stop_patience = 5  # stop if test loss hasn't improved for 5 epochs\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "stopped_early = False\n",
    "best_model_state = None\n",
    "LR_MIN_EXP = -4  # 10**-4 = 0.0001\n",
    "LR_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "\n",
    "# 2. Weight Decay (wd):\n",
    "#    Biasanya antara 1e-5 dan 1e-3.\n",
    "WD_MIN_EXP = -5  # 10**-5 = 0.00001\n",
    "WD_MAX_EXP = -3  # 10**-3 = 0.001\n",
    "# for i in range(20):\n",
    "    # lr = np.random.uniform(10**LR_MIN_EXP, 10**LR_MAX_EXP)\n",
    "    # wd = np.random.uniform(10**WD_MIN_EXP, 10**WD_MAX_EXP)\n",
    "    # print(f\"lr : {lr} | wd: {wd}\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(imgs)\n",
    "        loss = criterion(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            embeddings = model(imgs)\n",
    "            loss = criterion(embeddings, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if test_loss < best_loss - 1e-5:  # small threshold for improvement\n",
    "        best_loss = test_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= early_stop_patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        stopped_early = True\n",
    "        break\n",
    "\n",
    "# Optionally, load the best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "# Save the best model after training / early stopping\n",
    "save_path = r\"D:\\Code\\Kuliah\\Deep Learning\\tugas_4\\models\\mobilenetv2_arcface_diffmargin_xuexing_moreagument_best.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"âœ… Best model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab1de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deeplearning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
